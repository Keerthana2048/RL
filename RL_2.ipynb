{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTpQLujVQQGMGMkNl9KsdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keerthana2048/RL/blob/main/RL_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrwQZm5J7Jgj",
        "outputId": "7b169b31-9af2-4808-c4f8-997010016c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "state, info = env.reset()\n",
        "print(f\"Initial State : {state}\")\n",
        "\n",
        "for step in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    print(f\"\\nStep {step + 1}\")\n",
        "    print(f\"Action Taken : {action}\")\n",
        "\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    print(f\"Next State: {next_state}\")\n",
        "    print(f\"Reward : {reward}\")\n",
        "    print(f\"Terminated: {terminated}, Truncated : {truncated}\")\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(\"\\nEpisode finished - resetting environment.\")\n",
        "        state, info = env.reset()\n",
        "        print(f\"Now Initial State : {state}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffXukxlw8rmw",
        "outputId": "32a96bbf-1fb9-4de6-f968-3a34a99fef28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 188\n",
            "Action Taken : 1\n",
            "Next State: [-0.17444281 -0.7651093   0.20108406  1.3909961 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 189\n",
            "Action Taken : 0\n",
            "Next State: [-0.189745   -0.96208596  0.22890398  1.7392288 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [0.00803502 0.01320432 0.02494876 0.04163482]\n",
            "\n",
            "Step 190\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00829911  0.20795979  0.02578145 -0.2430733 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 191\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0124583   0.40270418  0.02091999 -0.5275139 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 192\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02051239  0.59752566  0.01036971 -0.8135321 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 193\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0324629   0.40226322 -0.00590093 -0.5176056 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 194\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04050817  0.20722485 -0.01625304 -0.22678797]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 195\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04465266  0.40257525 -0.0207888  -0.52455306]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 196\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05270417  0.5979835  -0.03127987 -0.8237136 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 197\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06466383  0.4033031  -0.04775413 -0.5410306 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 198\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0727299   0.20888375 -0.05857475 -0.2637684 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 199\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07690758  0.40479073 -0.06385011 -0.57433605]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 200\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08500339  0.600747   -0.07533684 -0.8864307 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 201\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09701832  0.40672413 -0.09306545 -0.61834997]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 202\n",
            "Action Taken : 0\n",
            "Next State: [ 0.10515281  0.2130169  -0.10543245 -0.35636827]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 203\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10941315  0.4094675  -0.11255982 -0.6803476 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 204\n",
            "Action Taken : 0\n",
            "Next State: [ 0.1176025   0.21607402 -0.12616676 -0.4251159 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 205\n",
            "Action Taken : 1\n",
            "Next State: [ 0.12192398  0.4127362  -0.13466908 -0.75475913]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 206\n",
            "Action Taken : 1\n",
            "Next State: [ 0.1301787   0.6094322  -0.14976427 -1.0866048 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 207\n",
            "Action Taken : 0\n",
            "Next State: [ 0.14236735  0.4165684  -0.17149636 -0.84441376]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 208\n",
            "Action Taken : 0\n",
            "Next State: [ 0.15069872  0.22414935 -0.18838464 -0.61019236]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 209\n",
            "Action Taken : 1\n",
            "Next State: [ 0.1551817  0.4213355 -0.2005885 -0.9557968]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 210\n",
            "Action Taken : 1\n",
            "Next State: [ 0.16360842  0.6185073  -0.21970442 -1.3042027 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.04431222  0.04570154 -0.02708309 -0.03936634]\n",
            "\n",
            "Step 211\n",
            "Action Taken : 1\n",
            "Next State: [-0.04339819  0.24120118 -0.02787042 -0.34046972]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 212\n",
            "Action Taken : 0\n",
            "Next State: [-0.03857417  0.04648664 -0.03467981 -0.05670417]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 213\n",
            "Action Taken : 0\n",
            "Next State: [-0.03764444 -0.14812134 -0.03581389  0.22483845]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 214\n",
            "Action Taken : 0\n",
            "Next State: [-0.04060686 -0.34271362 -0.03131713  0.5060127 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 215\n",
            "Action Taken : 1\n",
            "Next State: [-0.04746114 -0.14716467 -0.02119687  0.20362735]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 216\n",
            "Action Taken : 1\n",
            "Next State: [-0.05040443  0.0482539  -0.01712433 -0.09566607]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 217\n",
            "Action Taken : 1\n",
            "Next State: [-0.04943935  0.24361704 -0.01903765 -0.39370212]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 218\n",
            "Action Taken : 1\n",
            "Next State: [-0.04456701  0.43900388 -0.02691169 -0.692326  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 219\n",
            "Action Taken : 0\n",
            "Next State: [-0.03578693  0.24426545 -0.04075821 -0.40823522]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 220\n",
            "Action Taken : 0\n",
            "Next State: [-0.03090162  0.04974439 -0.04892292 -0.12867554]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 221\n",
            "Action Taken : 1\n",
            "Next State: [-0.02990674  0.2455318  -0.05149642 -0.43638286]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 222\n",
            "Action Taken : 0\n",
            "Next State: [-0.0249961   0.05117519 -0.06022408 -0.1603677 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 223\n",
            "Action Taken : 1\n",
            "Next State: [-0.0239726   0.24710532 -0.06343143 -0.47142524]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 224\n",
            "Action Taken : 1\n",
            "Next State: [-0.01903049  0.44306317 -0.07285994 -0.7834072 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 225\n",
            "Action Taken : 0\n",
            "Next State: [-0.01016923  0.2490141  -0.08852809 -0.5145078 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 226\n",
            "Action Taken : 0\n",
            "Next State: [-0.00518895  0.05524313 -0.09881824 -0.25098282]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 227\n",
            "Action Taken : 1\n",
            "Next State: [-0.00408408  0.25162715 -0.1038379  -0.57312703]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 228\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00094846  0.05810262 -0.11530044 -0.31487733]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 229\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00211051  0.25466204 -0.12159798 -0.6415821 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 230\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00720375  0.06142617 -0.13442962 -0.3895303 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 231\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00843228  0.25817448 -0.14222023 -0.7213935 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 232\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01359577  0.45494744 -0.1566481  -1.0552449 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 233\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02269472  0.6517594  -0.177753   -1.3927145 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 234\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0357299   0.45923916 -0.2056073  -1.1604689 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 235\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04491469  0.26730067 -0.22881667 -0.9386489 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.02408568 -0.03741858  0.03065181 -0.03759884]\n",
            "\n",
            "Step 236\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02333731 -0.23296636  0.02989984  0.26459527]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 237\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01867798 -0.03828366  0.03519174 -0.01850902]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 238\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01791231 -0.23389216  0.03482156  0.2850663 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 239\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01323446 -0.42949298  0.04052289  0.5885252 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 240\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0046446  -0.23496124  0.05229339  0.30887753]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 241\n",
            "Action Taken : 0\n",
            "Next State: [-5.4620705e-05 -4.3078777e-01  5.8470942e-02  6.1758304e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 242\n",
            "Action Taken : 0\n",
            "Next State: [-0.00867038 -0.62667567  0.0708226   0.92809343]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 243\n",
            "Action Taken : 0\n",
            "Next State: [-0.02120389 -0.8226787   0.08938447  1.2421653 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 244\n",
            "Action Taken : 0\n",
            "Next State: [-0.03765746 -1.018827    0.11422778  1.5614573 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 245\n",
            "Action Taken : 0\n",
            "Next State: [-0.058034   -1.2151152   0.14545693  1.8874807 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 246\n",
            "Action Taken : 0\n",
            "Next State: [-0.08233631 -1.4114883   0.18320654  2.2215436 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 247\n",
            "Action Taken : 0\n",
            "Next State: [-0.11056607 -1.6078253   0.22763741  2.5646822 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.02155843 -0.02432596 -0.01088921  0.04856788]\n",
            "\n",
            "Step 248\n",
            "Action Taken : 1\n",
            "Next State: [-0.02204495  0.17095043 -0.00991785 -0.2475307 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 249\n",
            "Action Taken : 0\n",
            "Next State: [-0.01862594 -0.02402849 -0.01486846  0.04200749]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 250\n",
            "Action Taken : 0\n",
            "Next State: [-0.01910651 -0.2189341  -0.01402831  0.32996243]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 251\n",
            "Action Taken : 1\n",
            "Next State: [-0.02348519 -0.0236153  -0.00742907  0.03288886]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 252\n",
            "Action Taken : 1\n",
            "Next State: [-0.02395749  0.1716124  -0.00677129 -0.26212874]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 253\n",
            "Action Taken : 1\n",
            "Next State: [-0.02052525  0.36683035 -0.01201386 -0.55693966]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 254\n",
            "Action Taken : 0\n",
            "Next State: [-0.01318864  0.1718791  -0.02315266 -0.2680659 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 255\n",
            "Action Taken : 0\n",
            "Next State: [-0.00975106 -0.02290491 -0.02851397  0.01722553]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 256\n",
            "Action Taken : 1\n",
            "Next State: [-0.01020916  0.17261411 -0.02816946 -0.28431576]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 257\n",
            "Action Taken : 1\n",
            "Next State: [-0.00675687  0.36812627 -0.03385578 -0.5857484 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 258\n",
            "Action Taken : 1\n",
            "Next State: [ 6.0565048e-04  5.6370568e-01 -4.5570746e-02 -8.8890105e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 259\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01187976  0.75941545 -0.06334877 -1.1955541 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 260\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02706807  0.56516826 -0.08725985 -0.92337984]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 261\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03837144  0.37132663 -0.10572745 -0.6593455 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 262\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04579797  0.17782259 -0.11891436 -0.40173626]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 263\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04935442  0.3744128  -0.12694909 -0.72941756]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 264\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05684268  0.18125282 -0.14153743 -0.47923207]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 265\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06046773 -0.01161706 -0.15112208 -0.23429343]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 266\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06023539  0.18530472 -0.15580794 -0.5705705 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 267\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06394149 -0.00732857 -0.16721936 -0.3307432 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 268\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06379492  0.18972988 -0.17383422 -0.67114156]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 269\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06758951  0.3867876  -0.18725705 -1.0131235 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 270\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07532527  0.5838466  -0.20751952 -1.3582771 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 271\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0870022   0.39184162 -0.23468506 -1.1370226 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.03632221 -0.00906032 -0.04442407 -0.0285084 ]\n",
            "\n",
            "Step 272\n",
            "Action Taken : 0\n",
            "Next State: [ 0.036141   -0.20351797 -0.04499424  0.24983393]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 273\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03207064 -0.39796945 -0.03999756  0.5279921 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 274\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02411125 -0.20230827 -0.02943772  0.22297892]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 275\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02006508 -0.39699736 -0.02497814  0.5062326 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 276\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01212514 -0.5917586  -0.01485349  0.7909405 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 277\n",
            "Action Taken : 1\n",
            "Next State: [ 2.8996507e-04 -3.9643589e-01  9.6532248e-04  4.9362198e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 278\n",
            "Action Taken : 1\n",
            "Next State: [-0.00763875 -0.20132756  0.01083776  0.20124345]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 279\n",
            "Action Taken : 1\n",
            "Next State: [-0.0116653  -0.00636227  0.01486263 -0.08800107]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 280\n",
            "Action Taken : 1\n",
            "Next State: [-0.01179255  0.18854351  0.01310261 -0.37595803]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 281\n",
            "Action Taken : 1\n",
            "Next State: [-0.00802168  0.38347694  0.00558345 -0.664481  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 282\n",
            "Action Taken : 1\n",
            "Next State: [-3.5214008e-04  5.7852077e-01 -7.7061704e-03 -9.5540065e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 283\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01121828  0.77374554 -0.02681418 -1.2504947 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 284\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02669319  0.9692007  -0.05182408 -1.5514544 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 285\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0460772   0.7747372  -0.08285317 -1.2753803 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 286\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06157194  0.9708123  -0.10836077 -1.592815  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 287\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08098819  1.1670406  -0.14021707 -1.9172268 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 288\n",
            "Action Taken : 0\n",
            "Next State: [ 0.104329    0.97367847 -0.17856161 -1.671119  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 289\n",
            "Action Taken : 1\n",
            "Next State: [ 0.12380257  1.1703691  -0.211984   -2.0136828 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.02881002  0.00926592  0.02131329  0.01251809]\n",
            "\n",
            "Step 290\n",
            "Action Taken : 1\n",
            "Next State: [-0.0286247   0.20407583  0.02156365 -0.27336478]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 291\n",
            "Action Taken : 1\n",
            "Next State: [-0.02454319  0.39888358  0.01609636 -0.55916923]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 292\n",
            "Action Taken : 1\n",
            "Next State: [-0.01656551  0.5937759   0.00491297 -0.84673774]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 293\n",
            "Action Taken : 0\n",
            "Next State: [-0.00469     0.3985873  -0.01202178 -0.55251396]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 294\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00328175  0.20363623 -0.02307206 -0.2636428 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 295\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00735447  0.00885106 -0.02834492  0.02167461]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 296\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0075315  -0.18585317 -0.02791142  0.30528134]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 297\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00381443  0.00965519 -0.0218058   0.00392815]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 298\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00400754 -0.18514736 -0.02172724  0.2896521 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 299\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00030459  0.01027756 -0.01593419 -0.0098034 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 300\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00051014 -0.18461229 -0.01613026  0.2778098 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 301\n",
            "Action Taken : 0\n",
            "Next State: [-0.00318211 -0.37950045 -0.01057407  0.5653619 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 302\n",
            "Action Taken : 0\n",
            "Next State: [-1.0772116e-02 -5.7447249e-01  7.3317369e-04  8.5469490e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 303\n",
            "Action Taken : 1\n",
            "Next State: [-0.02226156 -0.37936053  0.01782707  0.5622426 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 304\n",
            "Action Taken : 0\n",
            "Next State: [-0.02984877 -0.574728    0.02907192  0.8604882 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 305\n",
            "Action Taken : 0\n",
            "Next State: [-0.04134334 -0.77023363  0.04628169  1.1621686 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 306\n",
            "Action Taken : 1\n",
            "Next State: [-0.05674801 -0.575744    0.06952506  0.88434845]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 307\n",
            "Action Taken : 1\n",
            "Next State: [-0.06826289 -0.38163143  0.08721203  0.614307  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 308\n",
            "Action Taken : 0\n",
            "Next State: [-0.07589552 -0.5778568   0.09949817  0.93313426]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 309\n",
            "Action Taken : 0\n",
            "Next State: [-0.08745265 -0.77417016  0.11816085  1.2553521 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 310\n",
            "Action Taken : 0\n",
            "Next State: [-0.10293605 -0.97059     0.1432679   1.5825859 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 311\n",
            "Action Taken : 0\n",
            "Next State: [-0.12234785 -1.1670966   0.17491962  1.9163028 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 312\n",
            "Action Taken : 1\n",
            "Next State: [-0.14568979 -0.97423637  0.21324567  1.6825913 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00212569  0.02432078 -0.04215475  0.04723504]\n",
            "\n",
            "Step 313\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00261211  0.22002105 -0.04121005 -0.2584444 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 314\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00701253  0.41570637 -0.04637894 -0.5638355 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 315\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01532665  0.61144733 -0.05765565 -0.87076175]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 316\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0275556   0.417155   -0.07507088 -0.5967489 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 317\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0358987   0.61324275 -0.08700586 -0.9121022 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 318\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04816356  0.8094273  -0.1052479  -1.2308134 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 319\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0643521   1.0057337  -0.12986417 -1.5545294 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 320\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08446678  0.8123853  -0.16095476 -1.3050218 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 321\n",
            "Action Taken : 0\n",
            "Next State: [ 0.10071448  0.619628   -0.1870552  -1.0667396 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 322\n",
            "Action Taken : 1\n",
            "Next State: [ 0.11310704  0.8166654  -0.20838998 -1.4118141 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 323\n",
            "Action Taken : 1\n",
            "Next State: [ 0.12944035  1.0136709  -0.23662627 -1.7617533 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00231339 -0.00818762 -0.02670175 -0.00490744]\n",
            "\n",
            "Step 324\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00214963  0.1873069  -0.0267999  -0.30589405]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 325\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00589577 -0.0074231  -0.03291778 -0.02178218]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 326\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00574731  0.18815508 -0.03335343 -0.3246666 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 327\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00951041  0.38373566 -0.03984676 -0.6276784 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 328\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01718513  0.18919185 -0.05240033 -0.34780616]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 329\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02096896  0.38501844 -0.05935645 -0.6565415 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 330\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02866933  0.19077082 -0.07248728 -0.38312376]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 331\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03248475  0.3868431  -0.08014975 -0.6977524 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 332\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04022161  0.58297956 -0.0941048  -1.0145514 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 333\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0518812   0.38923007 -0.11439583 -0.7528391 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 334\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0596658   0.5857279  -0.12945262 -1.0792184 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 335\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07138036  0.78229934 -0.15103698 -1.4095613 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 336\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08702634  0.58933866 -0.17922822 -1.1676517 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 337\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09881312  0.7862812  -0.20258124 -1.5107448 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 338\n",
            "Action Taken : 0\n",
            "Next State: [ 0.11453874  0.594107   -0.23279615 -1.2875307 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.04220134  0.046248   -0.00520705  0.04800616]\n",
            "\n",
            "Step 339\n",
            "Action Taken : 1\n",
            "Next State: [-0.04127638  0.24144423 -0.00424692 -0.24631508]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 340\n",
            "Action Taken : 0\n",
            "Next State: [-0.0364475   0.04638319 -0.00917322  0.04502525]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 341\n",
            "Action Taken : 1\n",
            "Next State: [-0.03551983  0.24163547 -0.00827272 -0.25053772]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 342\n",
            "Action Taken : 1\n",
            "Next State: [-0.03068713  0.43687457 -0.01328347 -0.5458185 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 343\n",
            "Action Taken : 0\n",
            "Next State: [-0.02194963  0.24194176 -0.02419985 -0.25735033]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 344\n",
            "Action Taken : 0\n",
            "Next State: [-0.0171108   0.04717352 -0.02934685  0.02760244]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 345\n",
            "Action Taken : 1\n",
            "Next State: [-0.01616733  0.24270377 -0.0287948  -0.27419338]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 346\n",
            "Action Taken : 0\n",
            "Next State: [-0.01131325  0.04800424 -0.03427867  0.00927034]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 347\n",
            "Action Taken : 0\n",
            "Next State: [-0.01035317 -0.14660977 -0.03409326  0.2909439 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 348\n",
            "Action Taken : 0\n",
            "Next State: [-0.01328536 -0.3412294  -0.02827439  0.57268226]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 349\n",
            "Action Taken : 1\n",
            "Next State: [-0.02010995 -0.14572266 -0.01682074  0.27122778]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 350\n",
            "Action Taken : 1\n",
            "Next State: [-0.0230244   0.04963522 -0.01139619 -0.02671267]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 351\n",
            "Action Taken : 0\n",
            "Next State: [-0.0220317  -0.14532146 -0.01193044  0.26235297]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 352\n",
            "Action Taken : 1\n",
            "Next State: [-0.02493813  0.04996874 -0.00668338 -0.03406895]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 353\n",
            "Action Taken : 1\n",
            "Next State: [-0.02393875  0.2451859  -0.00736476 -0.32885304]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 354\n",
            "Action Taken : 1\n",
            "Next State: [-0.01903504  0.4404119  -0.01394182 -0.62384933]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 355\n",
            "Action Taken : 0\n",
            "Next State: [-0.0102268   0.24548736 -0.02641881 -0.3355897 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 356\n",
            "Action Taken : 1\n",
            "Next State: [-0.00531705  0.44097513 -0.0331306  -0.6364853 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 357\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00350245  0.24633051 -0.04586031 -0.35441718]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 358\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00842906  0.44207352 -0.05294865 -0.6612012 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 359\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01727053  0.6378907  -0.06617267 -0.97007495]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 360\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03002835  0.4437164  -0.08557417 -0.6988915 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 361\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03890267  0.63991404 -0.099552   -1.0172392 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 362\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05170095  0.8362121  -0.11989678 -1.3394483 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 363\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06842519  0.6427863  -0.14668575 -1.0865579 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 364\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08128092  0.8395061  -0.16841692 -1.4214398 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 365\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09807105  0.6468198  -0.19684571 -1.1857805 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 366\n",
            "Action Taken : 0\n",
            "Next State: [ 0.11100744  0.45471787 -0.22056131 -0.960692  ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00034866  0.00041383 -0.00755321 -0.01602917]\n",
            "\n",
            "Step 367\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00035694 -0.19459899 -0.0078738   0.2742611 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 368\n",
            "Action Taken : 1\n",
            "Next State: [-0.00353504  0.00063442 -0.00238858 -0.02089481]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 369\n",
            "Action Taken : 0\n",
            "Next State: [-0.00352235 -0.1944532  -0.00280647  0.27103353]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 370\n",
            "Action Taken : 0\n",
            "Next State: [-0.00741142 -0.38953498  0.0026142   0.56283   ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 371\n",
            "Action Taken : 1\n",
            "Next State: [-0.01520212 -0.19444981  0.0138708   0.27097178]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 372\n",
            "Action Taken : 0\n",
            "Next State: [-0.01909111 -0.38976693  0.01929023  0.56799716]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 373\n",
            "Action Taken : 1\n",
            "Next State: [-0.02688645 -0.1949208   0.03065018  0.2814533 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 374\n",
            "Action Taken : 1\n",
            "Next State: [-0.03078487 -0.00024914  0.03627924 -0.00140728]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 375\n",
            "Action Taken : 1\n",
            "Next State: [-0.03078985  0.19433424  0.0362511  -0.28242654]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 376\n",
            "Action Taken : 1\n",
            "Next State: [-0.02690317  0.38892087  0.03060257 -0.5634592 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 377\n",
            "Action Taken : 1\n",
            "Next State: [-0.01912475  0.58360034  0.01933338 -0.84634596]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 378\n",
            "Action Taken : 0\n",
            "Next State: [-0.00745274  0.38822004  0.00240646 -0.54764664]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 379\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00031166  0.19306438 -0.00854647 -0.25420645]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 380\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00417295  0.3883073  -0.0136306  -0.54957277]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 381\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01193909  0.58361804 -0.02462205 -0.84651893]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 382\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02361145  0.7790671  -0.04155244 -1.1468418 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 383\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0391928   0.9747063  -0.06448927 -1.4522601 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 384\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05868692  1.1705585  -0.09353448 -1.7643744 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 385\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08209809  1.3666052  -0.12882197 -2.084618  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 386\n",
            "Action Taken : 0\n",
            "Next State: [ 0.10943019  1.1729995  -0.17051433 -1.834385  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 387\n",
            "Action Taken : 0\n",
            "Next State: [ 0.13289018  0.9801251  -0.20720203 -1.5991578 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 388\n",
            "Action Taken : 0\n",
            "Next State: [ 0.15249269  0.7879722  -0.23918518 -1.377576  ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.0067123  -0.00107882 -0.00901121 -0.02603018]\n",
            "\n",
            "Step 389\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00669072 -0.19607039 -0.00953182  0.26379603]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 390\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00276931 -0.00081369 -0.0042559  -0.03187802]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 391\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00275304  0.19436903 -0.00489346 -0.32590067]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 392\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00664042 -0.0006829  -0.01141147 -0.03476495]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 393\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00662676  0.19460082 -0.01210677 -0.33102635]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 394\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01051878  0.389893   -0.0187273  -0.62750244]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 395\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01831664  0.19503736 -0.03127734 -0.34077576]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 396\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02221739  0.39059004 -0.03809286 -0.64315534]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 397\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03002919  0.58622164 -0.05095597 -0.9475865 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 398\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04175362  0.3918215  -0.0699077  -0.67133933]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 399\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04959005  0.58784205 -0.08333448 -0.9851881 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 400\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06134689  0.3939292  -0.10303824 -0.71980023]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 401\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06922548  0.20037238 -0.11743425 -0.46124452]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 402\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07323293  0.007089   -0.12665914 -0.20776266]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 403\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0733747   0.2037731  -0.13081439 -0.5375638 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 404\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07745016  0.40046814 -0.14156567 -0.8684353 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 405\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08545953  0.59720296 -0.15893437 -1.2020669 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 406\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09740359  0.4044523  -0.18297571 -0.96311516]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 407\n",
            "Action Taken : 0\n",
            "Next State: [ 0.10549263  0.2121974  -0.20223802 -0.73304206]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 408\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10973658  0.40945366 -0.21689886 -1.0819497 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.01252887  0.03260066  0.03470673 -0.03410079]\n",
            "\n",
            "Step 409\n",
            "Action Taken : 0\n",
            "Next State: [-0.01187686 -0.16300136  0.03402472  0.26932728]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 410\n",
            "Action Taken : 0\n",
            "Next State: [-0.01513689 -0.35859194  0.03941126  0.5725447 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 411\n",
            "Action Taken : 0\n",
            "Next State: [-0.02230873 -0.5542437   0.05086216  0.87737834]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 412\n",
            "Action Taken : 0\n",
            "Next State: [-0.0333936  -0.7500186   0.06840973  1.185608  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 413\n",
            "Action Taken : 0\n",
            "Next State: [-0.04839398 -0.94595784  0.09212188  1.4989262 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 414\n",
            "Action Taken : 0\n",
            "Next State: [-0.06731313 -1.1420704   0.12210041  1.8188932 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 415\n",
            "Action Taken : 1\n",
            "Next State: [-0.09015454 -0.9484992   0.15847827  1.5665065 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 416\n",
            "Action Taken : 1\n",
            "Next State: [-0.10912453 -0.75558597  0.1898084   1.3271606 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 417\n",
            "Action Taken : 0\n",
            "Next State: [-0.12423624 -0.95252794  0.21635161  1.6727374 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [0.04973483 0.01655721 0.03382136 0.0386427 ]\n",
            "\n",
            "Step 418\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05006598 -0.179033    0.03459421  0.34180182]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 419\n",
            "Action Taken : 1\n",
            "Next State: [0.04648532 0.01558012 0.04143025 0.06022546]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 420\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04679692 -0.1801106   0.04263476  0.36568668]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 421\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0431947  -0.3758117   0.04994849  0.67150235]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 422\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03567847 -0.18141837  0.06337854  0.3949548 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 423\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0320501  -0.3773797   0.07127763  0.7069274 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 424\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02450251 -0.18331379  0.08541618  0.43750563]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 425\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02083623 -0.37953436  0.09416629  0.75584525]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 426\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01324555 -0.57581943  0.1092832   1.0766125 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 427\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00172916 -0.38229743  0.13081545  0.8201264 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 428\n",
            "Action Taken : 0\n",
            "Next State: [-0.00591679 -0.5789437   0.14721797  1.1509258 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 429\n",
            "Action Taken : 0\n",
            "Next State: [-0.01749566 -0.77564734  0.1702365   1.4859155 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 430\n",
            "Action Taken : 0\n",
            "Next State: [-0.03300861 -0.9723857   0.19995481  1.8265654 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 431\n",
            "Action Taken : 0\n",
            "Next State: [-0.05245633 -1.1690851   0.2364861   2.1741316 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00420508  0.03059244 -0.01837958  0.0043556 ]\n",
            "\n",
            "Step 432\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00481693  0.2259731  -0.01829247 -0.29406917]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 433\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00933639  0.03111665 -0.02417385 -0.0072111 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 434\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00995872 -0.16365042 -0.02431807  0.27774772]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 435\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00668571 -0.35841715 -0.01876312  0.56266266]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 436\n",
            "Action Taken : 0\n",
            "Next State: [-4.8262856e-04 -5.5327088e-01 -7.5098639e-03  8.4937572e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 437\n",
            "Action Taken : 0\n",
            "Next State: [-0.01154805 -0.7482896   0.00947765  1.1396877 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 438\n",
            "Action Taken : 0\n",
            "Next State: [-0.02651384 -0.94353414  0.0322714   1.4353278 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 439\n",
            "Action Taken : 1\n",
            "Next State: [-0.04538452 -0.7488247   0.06097796  1.1529018 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 440\n",
            "Action Taken : 0\n",
            "Next State: [-0.06036101 -0.9446868   0.08403599  1.4640652 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 441\n",
            "Action Taken : 0\n",
            "Next State: [-0.07925475 -1.1407316   0.1133173   1.7817724 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 442\n",
            "Action Taken : 1\n",
            "Next State: [-0.10206939 -0.94705194  0.14895275  1.52636   ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 443\n",
            "Action Taken : 0\n",
            "Next State: [-0.12101042 -1.1436245   0.17947994  1.8615844 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 444\n",
            "Action Taken : 1\n",
            "Next State: [-0.14388292 -0.9508676   0.21671164  1.6295776 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.0017022   0.02667696 -0.00432186 -0.00119017]\n",
            "\n",
            "Step 445\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00223574  0.22186063 -0.00434566 -0.29523355]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 446\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00667296  0.0268009  -0.01025033 -0.00392434]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 447\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00720897  0.22206834 -0.01032882 -0.29982367]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 448\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01165034  0.02709513 -0.01632529 -0.01041607]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 449\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01219224 -0.16778894 -0.01653362  0.27707165]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 450\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00883647  0.02756494 -0.01099218 -0.02077977]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 451\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00938776 -0.16739766 -0.01140778  0.26841483]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 452\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00603981  0.02788522 -0.00603948 -0.02784425]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 453\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00659752 -0.1671496  -0.00659637  0.26292706]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 454\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00325452  0.02806588 -0.00133783 -0.03182912]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 455\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00381584 -0.16703686 -0.00197441  0.2604314 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 456\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0004751   0.02811322  0.00323422 -0.03287362]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 457\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00103737  0.22318864  0.00257675 -0.32453436]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 458\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00550114  0.0280301  -0.00391394 -0.03103996]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 459\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00606174  0.22320797 -0.00453474 -0.3249552 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 460\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0105259   0.02815087 -0.01103384 -0.03370578]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 461\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01108892  0.22342929 -0.01170796 -0.32984948]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 462\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01555751  0.41871595 -0.01830495 -0.62620145]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 463\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02393183  0.22385423 -0.03082898 -0.33933917]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 464\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02840891  0.0291842  -0.03761576 -0.05653517]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 465\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02899259  0.22482474 -0.03874646 -0.36084482]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 466\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03348909  0.42047542 -0.04596336 -0.6654892 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 467\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0418986   0.6162055  -0.05927314 -0.97228277]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 468\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05422271  0.8120706  -0.0787188  -1.2829806 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 469\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07046412  0.6180345  -0.10437841 -1.0159471 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 470\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08282481  0.81438166 -0.12469735 -1.3394965 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 471\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09911244  1.0108334  -0.15148728 -1.668452  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 472\n",
            "Action Taken : 0\n",
            "Next State: [ 0.11932911  0.8177628  -0.18485633 -1.4265299 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 473\n",
            "Action Taken : 0\n",
            "Next State: [ 0.13568437  0.6253427  -0.21338692 -1.196856  ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.03684089 -0.02264714  0.04274675 -0.03672601]\n",
            "\n",
            "Step 474\n",
            "Action Taken : 1\n",
            "Next State: [-0.03729383  0.17183657  0.04201223 -0.31562138]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 475\n",
            "Action Taken : 0\n",
            "Next State: [-0.0338571  -0.02385784  0.0356998  -0.0099908 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 476\n",
            "Action Taken : 0\n",
            "Next State: [-0.03433425 -0.21947311  0.03549998  0.29373866]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 477\n",
            "Action Taken : 0\n",
            "Next State: [-0.03872371 -0.41508275  0.04137475  0.59740305]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 478\n",
            "Action Taken : 0\n",
            "Next State: [-0.04702537 -0.6107585   0.05332281  0.9028262 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 479\n",
            "Action Taken : 1\n",
            "Next State: [-0.05924054 -0.41639784  0.07137934  0.62736905]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 480\n",
            "Action Taken : 1\n",
            "Next State: [-0.0675685  -0.22234093  0.08392672  0.35799262]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 481\n",
            "Action Taken : 0\n",
            "Next State: [-0.07201532 -0.4185495   0.09108657  0.67591506]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 482\n",
            "Action Taken : 0\n",
            "Next State: [-0.0803863  -0.6148112   0.10460487  0.99582964]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 483\n",
            "Action Taken : 0\n",
            "Next State: [-0.09268253 -0.81116474  0.12452146  1.3194478 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 484\n",
            "Action Taken : 1\n",
            "Next State: [-0.10890582 -0.6178175   0.15091042  1.0681872 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 485\n",
            "Action Taken : 0\n",
            "Next State: [-0.12126217 -0.81457824  0.17227417  1.4041735 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 486\n",
            "Action Taken : 1\n",
            "Next State: [-0.13755374 -0.6219628   0.20035763  1.1699256 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 487\n",
            "Action Taken : 1\n",
            "Next State: [-0.14999299 -0.42992878  0.22375615  0.94614863]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.0297952  -0.01381212 -0.0398779  -0.03185474]\n",
            "\n",
            "Step 488\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02951896  0.18185833 -0.04051499 -0.33684808]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 489\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03315613  0.37753272 -0.04725196 -0.64202696]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 490\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04070678  0.5732804  -0.06009249 -0.9492076 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 491\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05217239  0.37901664 -0.07907665 -0.6759945 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 492\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05975272  0.57514316 -0.09259654 -0.99248934]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 493\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07125559  0.7713739  -0.11244632 -1.3127589 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 494\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08668306  0.5778407  -0.1387015  -1.0572821 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 495\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09823988  0.38480172 -0.15984714 -0.81115204]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 496\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10593591  0.5817101  -0.17607018 -1.1495444 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 497\n",
            "Action Taken : 1\n",
            "Next State: [ 0.11757011  0.77863735 -0.19906108 -1.491866  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 498\n",
            "Action Taken : 1\n",
            "Next State: [ 0.13314286  0.9755465  -0.22889839 -1.8395354 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00271541 -0.00395812 -0.00852818  0.02696092]\n",
            "\n",
            "Step 499\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00263625  0.19128509 -0.00798896 -0.2684005 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 500\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00646195 -0.00372194 -0.01335697  0.02175199]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 501\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00638751 -0.19864982 -0.01292193  0.3101909 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 502\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00241451 -0.00334617 -0.00671811  0.01346093]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 503\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00234759 -0.19837113 -0.00644889  0.30401668]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 504\n",
            "Action Taken : 1\n",
            "Next State: [-0.00161983 -0.00315787 -0.00036856  0.00930691]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 505\n",
            "Action Taken : 1\n",
            "Next State: [-1.6829917e-03  1.9196936e-01 -1.8241994e-04 -2.8349227e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 506\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0021564   0.3870939  -0.00585227 -0.57623273]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 507\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00989827  0.19205448 -0.01737692 -0.28539917]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 508\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01373936  0.3874199  -0.0230849  -0.5835116 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 509\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02148776  0.19262883 -0.03475514 -0.2981892 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 510\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02534034 -0.00198091 -0.04071892 -0.01666684]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 511\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02530072  0.19370063 -0.04105226 -0.3219139 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 512\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02917473  0.3893824  -0.04749053 -0.6272552 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 513\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03696238  0.5851339  -0.06003564 -0.9345084 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 514\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04866506  0.39087096 -0.07872581 -0.6612788 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 515\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05648248  0.19692759 -0.09195139 -0.39438632]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 516\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06042103  0.39322582 -0.09983911 -0.71458536]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 517\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06828555  0.19961728 -0.11413082 -0.45492274]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 518\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07227789  0.39615238 -0.12322927 -0.78128904]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 519\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08020094  0.59273326 -0.13885505 -1.1100621 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 520\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0920556   0.3996814  -0.1610563  -0.86396384]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 521\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10004923  0.5965861  -0.17833558 -1.2026446 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 522\n",
            "Action Taken : 1\n",
            "Next State: [ 0.11198095  0.79350805 -0.20238847 -1.545496  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 523\n",
            "Action Taken : 0\n",
            "Next State: [ 0.12785111  0.60130984 -0.23329838 -1.3221799 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.02985626 -0.04460894 -0.04818273 -0.0387518 ]\n",
            "\n",
            "Step 524\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02896409  0.15116964 -0.04895777 -0.34623912]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 525\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03198748  0.34695256 -0.05588255 -0.65394944]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 526\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03892653  0.15265144 -0.06896154 -0.37937364]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 527\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04197956  0.34868148 -0.07654902 -0.69297844]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 528\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04895319  0.5447772  -0.09040858 -1.0087441 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 529\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05984873  0.35097077 -0.11058346 -0.7457656 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 530\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06686815  0.5474308  -0.12549877 -1.071101  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 531\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07781676  0.3541713  -0.1469208  -0.82029146]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 532\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08490019  0.16133252 -0.16332662 -0.5771891 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 533\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08812684 -0.03116896 -0.1748704  -0.34008443]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 534\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08750346  0.16595384 -0.1816721  -0.68240947]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 535\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09082254 -0.02624263 -0.19532028 -0.45197752]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 536\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09029768  0.1710274  -0.20435983 -0.79931587]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 537\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09371824  0.36827835 -0.22034615 -1.1486999 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [0.00176084 0.00521413 0.02824532 0.00650909]\n",
            "\n",
            "Step 538\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00186512 -0.19030127  0.0283755   0.30796823]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 539\n",
            "Action Taken : 0\n",
            "Next State: [-0.0019409  -0.38581583  0.03453486  0.6094633 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 540\n",
            "Action Taken : 1\n",
            "Next State: [-0.00965722 -0.19119321  0.04672413  0.32785466]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 541\n",
            "Action Taken : 0\n",
            "Next State: [-0.01348109 -0.38694814  0.05328123  0.6348985 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 542\n",
            "Action Taken : 0\n",
            "Next State: [-0.02122005 -0.5827712   0.06597919  0.94387347]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 543\n",
            "Action Taken : 1\n",
            "Next State: [-0.03287547 -0.3885972   0.08485667  0.67263   ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 544\n",
            "Action Taken : 0\n",
            "Next State: [-0.04064741 -0.58478975  0.09830926  0.9907779 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 545\n",
            "Action Taken : 1\n",
            "Next State: [-0.05234321 -0.39111117  0.11812482  0.7305192 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 546\n",
            "Action Taken : 0\n",
            "Next State: [-0.06016543 -0.58765036  0.13273521  1.0579215 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 547\n",
            "Action Taken : 0\n",
            "Next State: [-0.07191844 -0.7842572   0.15389363  1.3891473 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 548\n",
            "Action Taken : 1\n",
            "Next State: [-0.08760358 -0.5913507   0.18167658  1.1482736 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 549\n",
            "Action Taken : 1\n",
            "Next State: [-0.0994306  -0.3990043   0.20464206  0.917622  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 550\n",
            "Action Taken : 1\n",
            "Next State: [-0.10741068 -0.20714878  0.22299449  0.6955894 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.04895463  0.03469956  0.03005344  0.04042926]\n",
            "\n",
            "Step 551\n",
            "Action Taken : 1\n",
            "Next State: [-0.04826064  0.22937794  0.03086202 -0.24262208]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 552\n",
            "Action Taken : 0\n",
            "Next State: [-0.04367308  0.03382907  0.02600958  0.05963356]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 553\n",
            "Action Taken : 1\n",
            "Next State: [-0.0429965   0.22856863  0.02720225 -0.22473101]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 554\n",
            "Action Taken : 0\n",
            "Next State: [-0.03842513  0.03306866  0.02270763  0.0764069 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 555\n",
            "Action Taken : 0\n",
            "Next State: [-0.03776376 -0.16237134  0.02423577  0.3761668 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 556\n",
            "Action Taken : 0\n",
            "Next State: [-0.04101118 -0.35782897  0.03175911  0.6763918 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 557\n",
            "Action Taken : 1\n",
            "Next State: [-0.04816776 -0.16316238  0.04528694  0.39387473]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 558\n",
            "Action Taken : 1\n",
            "Next State: [-0.05143101  0.03128866  0.05316444  0.11580703]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 559\n",
            "Action Taken : 0\n",
            "Next State: [-0.05080524 -0.16455315  0.05548058  0.42477766]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 560\n",
            "Action Taken : 0\n",
            "Next State: [-0.0540963  -0.3604153   0.06397613  0.7344218 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 561\n",
            "Action Taken : 1\n",
            "Next State: [-0.06130461 -0.1662328   0.07866456  0.46254006]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 562\n",
            "Action Taken : 1\n",
            "Next State: [-0.06462926  0.02769441  0.08791537  0.19565235]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 563\n",
            "Action Taken : 0\n",
            "Next State: [-0.06407537 -0.1685679   0.09182841  0.5147227 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 564\n",
            "Action Taken : 1\n",
            "Next State: [-0.06744673  0.025149    0.10212287  0.25233123]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 565\n",
            "Action Taken : 0\n",
            "Next State: [-0.06694375 -0.17127168  0.10716949  0.57539916]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 566\n",
            "Action Taken : 1\n",
            "Next State: [-0.07036918  0.02219763  0.11867747  0.31830773]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 567\n",
            "Action Taken : 1\n",
            "Next State: [-0.06992523  0.215447    0.12504363  0.06528196]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 568\n",
            "Action Taken : 1\n",
            "Next State: [-0.06561629  0.40857497  0.12634927 -0.18548107]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 569\n",
            "Action Taken : 1\n",
            "Next State: [-0.05744479  0.601684    0.12263965 -0.4357876 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 570\n",
            "Action Taken : 0\n",
            "Next State: [-0.04541111  0.40505874  0.1139239  -0.10709924]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 571\n",
            "Action Taken : 1\n",
            "Next State: [-0.03730994  0.59837943  0.11178191 -0.3617793 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 572\n",
            "Action Taken : 0\n",
            "Next State: [-0.02534235  0.40186086  0.10454633 -0.03604569]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 573\n",
            "Action Taken : 0\n",
            "Next State: [-0.01730513  0.20540714  0.10382541  0.28770658]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 574\n",
            "Action Taken : 0\n",
            "Next State: [-0.01319699  0.00896962  0.10957955  0.611246  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 575\n",
            "Action Taken : 1\n",
            "Next State: [-0.0130176   0.20240317  0.12180447  0.3549879 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 576\n",
            "Action Taken : 0\n",
            "Next State: [-0.00896953  0.00577893  0.12890422  0.6834611 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 577\n",
            "Action Taken : 1\n",
            "Next State: [-0.00885395  0.1988977   0.14257345  0.4339793 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 578\n",
            "Action Taken : 1\n",
            "Next State: [-0.004876    0.39174375  0.15125303  0.18941998]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 579\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00295887  0.58441466  0.15504143 -0.05198776]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 580\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01464717  0.7770129   0.15400168 -0.29202008]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 581\n",
            "Action Taken : 0\n",
            "Next State: [0.03018743 0.58006895 0.14816128 0.04499742]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 582\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04178881  0.77279     0.14906122 -0.19751681]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 583\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0572446   0.96550035  0.14511089 -0.43971503]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 584\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07655461  1.1583025   0.13631658 -0.68336576]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 585\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09972066  1.3512949   0.12264927 -0.93021554]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 586\n",
            "Action Taken : 1\n",
            "Next State: [ 0.12674657  1.544567    0.10404496 -1.1819775 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 587\n",
            "Action Taken : 0\n",
            "Next State: [ 0.1576379   1.3482602   0.08040541 -0.8585755 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 588\n",
            "Action Taken : 0\n",
            "Next State: [ 0.18460311  1.1521404   0.0632339  -0.54173255]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 589\n",
            "Action Taken : 1\n",
            "Next State: [ 0.20764591  1.3463192   0.05239925 -0.81384027]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 590\n",
            "Action Taken : 0\n",
            "Next State: [ 0.23457229  1.1505203   0.03612245 -0.5051467 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 591\n",
            "Action Taken : 0\n",
            "Next State: [ 0.2575827   0.95490843  0.02601951 -0.20130259]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 592\n",
            "Action Taken : 0\n",
            "Next State: [0.27668086 0.75942415 0.02199346 0.09947337]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 593\n",
            "Action Taken : 1\n",
            "Next State: [ 0.29186934  0.9542241   0.02398293 -0.18619034]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 594\n",
            "Action Taken : 1\n",
            "Next State: [ 0.31095383  1.1489949   0.02025912 -0.47121215]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 595\n",
            "Action Taken : 1\n",
            "Next State: [ 0.33393374  1.3438249   0.01083488 -0.7574414 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 596\n",
            "Action Taken : 1\n",
            "Next State: [ 0.36081022  1.5387958  -0.00431395 -1.0466954 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 597\n",
            "Action Taken : 0\n",
            "Next State: [ 0.39158615  1.3437314  -0.02524786 -0.75536966]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 598\n",
            "Action Taken : 0\n",
            "Next State: [ 0.4184608   1.1489664  -0.04035525 -0.4707374 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 599\n",
            "Action Taken : 0\n",
            "Next State: [ 0.4414401   0.9544371  -0.04977    -0.19104213]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 600\n",
            "Action Taken : 0\n",
            "Next State: [ 0.46052885  0.76006114 -0.05359084  0.08553438]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 601\n",
            "Action Taken : 0\n",
            "Next State: [ 0.47573006  0.5657467  -0.05188015  0.36083943]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 602\n",
            "Action Taken : 0\n",
            "Next State: [ 0.487045    0.3713991  -0.04466337  0.6367227 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 603\n",
            "Action Taken : 0\n",
            "Next State: [ 0.49447298  0.17692757 -0.03192892  0.9150124 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 604\n",
            "Action Taken : 0\n",
            "Next State: [ 0.49801153 -0.01774835 -0.01362867  1.197492  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 605\n",
            "Action Taken : 1\n",
            "Next State: [0.49765658 0.1775473  0.01032117 0.90056896]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 606\n",
            "Action Taken : 0\n",
            "Next State: [ 0.50120753 -0.01771297  0.02833255  1.1964781 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 607\n",
            "Action Taken : 1\n",
            "Next State: [0.50085324 0.177031   0.05226212 0.91280806]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 608\n",
            "Action Taken : 1\n",
            "Next State: [0.5043939  0.37140846 0.07051828 0.63699806]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 609\n",
            "Action Taken : 0\n",
            "Next State: [0.51182204 0.17537762 0.08325824 0.9510287 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 610\n",
            "Action Taken : 1\n",
            "Next State: [0.5153296  0.3692863  0.10227881 0.6856229 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 611\n",
            "Action Taken : 1\n",
            "Next State: [0.52271533 0.56285083 0.11599127 0.426811  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 612\n",
            "Action Taken : 1\n",
            "Next State: [0.5339723  0.75615525 0.12452749 0.17282768]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 613\n",
            "Action Taken : 1\n",
            "Next State: [ 0.54909545  0.94929534  0.12798405 -0.0781225 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 614\n",
            "Action Taken : 0\n",
            "Next State: [0.5680814  0.7525931  0.1264216  0.25204232]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 615\n",
            "Action Taken : 1\n",
            "Next State: [0.5831332  0.9457044  0.13146244 0.00175609]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 616\n",
            "Action Taken : 0\n",
            "Next State: [0.6020473  0.7489661  0.13149756 0.33285588]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 617\n",
            "Action Taken : 1\n",
            "Next State: [0.6170266  0.9419952  0.13815469 0.08436092]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 618\n",
            "Action Taken : 0\n",
            "Next State: [0.6358665 0.7451911 0.1398419 0.4172427]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 619\n",
            "Action Taken : 1\n",
            "Next State: [0.65077037 0.938083   0.14818676 0.17170897]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 620\n",
            "Action Taken : 0\n",
            "Next State: [0.669532   0.7411851  0.15162092 0.5072265 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 621\n",
            "Action Taken : 1\n",
            "Next State: [0.68435574 0.93388206 0.16176546 0.2659032 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 622\n",
            "Action Taken : 0\n",
            "Next State: [0.7030333  0.73686546 0.16708353 0.6049217 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 623\n",
            "Action Taken : 0\n",
            "Next State: [0.71777064 0.5398497  0.17918196 0.94522417]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 624\n",
            "Action Taken : 1\n",
            "Next State: [0.72856766 0.732165   0.19808644 0.7137679 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 625\n",
            "Action Taken : 1\n",
            "Next State: [0.743211   0.9240738  0.2123618  0.48939112]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.01724237 -0.0285706  -0.00013265  0.02336449]\n",
            "\n",
            "Step 626\n",
            "Action Taken : 1\n",
            "Next State: [-0.01781378  0.16655326  0.00033464 -0.2693603 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 627\n",
            "Action Taken : 1\n",
            "Next State: [-0.01448272  0.36167043 -0.00505257 -0.56193763]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 628\n",
            "Action Taken : 0\n",
            "Next State: [-0.00724931  0.16661975 -0.01629132 -0.2708508 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 629\n",
            "Action Taken : 1\n",
            "Next State: [-0.00391691  0.36197034 -0.02170834 -0.56862724]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 630\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00332249  0.5573899  -0.03308088 -0.86806935]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 631\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01447029  0.752946   -0.05044227 -1.170967  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 632\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02952921  0.55851495 -0.07386161 -0.8945151 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 633\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04069951  0.36446816 -0.09175191 -0.62593406]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 634\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04798887  0.17073867 -0.10427059 -0.36349937]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 635\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05140365  0.36717606 -0.11154058 -0.6871551 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 636\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05874717  0.563655   -0.12528369 -1.0127672 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 637\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07002027  0.37040684 -0.14553903 -0.76190394]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 638\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07742841  0.5672017  -0.1607771  -1.0966129 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 639\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08877244  0.3745195  -0.18270937 -0.8583822 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 640\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09626283  0.18229365 -0.19987701 -0.628261  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 641\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0999087  -0.00956056 -0.21244223 -0.4045825 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.00746863  0.02324094  0.01106148 -0.0422647 ]\n",
            "\n",
            "Step 642\n",
            "Action Taken : 1\n",
            "Next State: [-0.00700381  0.21820255  0.01021618 -0.3314372 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 643\n",
            "Action Taken : 1\n",
            "Next State: [-0.00263976  0.4131776   0.00358744 -0.620881  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 644\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00562379  0.21800573 -0.00883018 -0.3270704 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 645\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00998391  0.41325226 -0.01537159 -0.62252486]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 646\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01824895  0.2183483  -0.02782209 -0.3347225 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 647\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02261592  0.41385496 -0.03451654 -0.6360476 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 648\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03089302  0.60944086 -0.04723749 -0.9393976 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 649\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04308183  0.41498643 -0.06602544 -0.66192394]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 650\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05138156  0.22084226 -0.07926392 -0.3907396 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 651\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05579841  0.02692948 -0.08707871 -0.12406287]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 652\n",
            "Action Taken : 0\n",
            "Next State: [ 0.056337   -0.16684401 -0.08955996  0.13992727]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 653\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05300012 -0.36057672 -0.08676142  0.4030662 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 654\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04578858 -0.55436784 -0.0787001   0.6671837 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 655\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03470122 -0.74831223 -0.06535643  0.9340858 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 656\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01973498 -0.5523724  -0.04667471  0.6216024 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 657\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00868753 -0.7468125  -0.03424266  0.8992276 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 658\n",
            "Action Taken : 1\n",
            "Next State: [-0.00624872 -0.55124366 -0.01625811  0.5959809 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 659\n",
            "Action Taken : 0\n",
            "Next State: [-0.01727359 -0.74613434 -0.00433849  0.88349867]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 660\n",
            "Action Taken : 1\n",
            "Next State: [-0.03219628 -0.55095375  0.01333148  0.589455  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 661\n",
            "Action Taken : 0\n",
            "Next State: [-0.04321535 -0.7462598   0.02512058  0.8863074 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 662\n",
            "Action Taken : 0\n",
            "Next State: [-0.05814055 -0.94171363  0.04284673  1.1867802 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 663\n",
            "Action Taken : 1\n",
            "Next State: [-0.07697482 -0.74717265  0.06658234  0.9078297 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 664\n",
            "Action Taken : 0\n",
            "Next State: [-0.09191827 -0.9431297   0.08473893  1.2206748 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 665\n",
            "Action Taken : 1\n",
            "Next State: [-0.11078086 -0.7491958   0.10915243  0.9557012 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 666\n",
            "Action Taken : 0\n",
            "Next State: [-0.12576479 -0.9456032   0.12826645  1.280586  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 667\n",
            "Action Taken : 0\n",
            "Next State: [-0.14467685 -1.1421046   0.15387817  1.6105239 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 668\n",
            "Action Taken : 1\n",
            "Next State: [-0.16751894 -0.94909877  0.18608865  1.3694978 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 669\n",
            "Action Taken : 1\n",
            "Next State: [-0.18650092 -0.7567277   0.21347861  1.1403178 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [0.03175338 0.03110437 0.04478318 0.01877441]\n",
            "\n",
            "Step 670\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03237546  0.22555642  0.04515867 -0.25944936]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 671\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03688659  0.4200056   0.03996968 -0.5375536 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 672\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0452867   0.22434515  0.02921861 -0.23254941]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 673\n",
            "Action Taken : 0\n",
            "Next State: [0.04977361 0.02881814 0.02456762 0.06920495]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 674\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05034997 -0.16664726  0.02595172  0.36953673]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 675\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04701702 -0.36212814  0.03334245  0.67028826]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 676\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03977446 -0.16748522  0.04674822  0.38828704]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 677\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03642476 -0.36323848  0.05451396  0.6953351 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 678\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02915999 -0.1689133   0.06842066  0.42029953]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 679\n",
            "Action Taken : 1\n",
            "Next State: [0.02578172 0.02517585 0.07682665 0.14994694]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 680\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02628524  0.21911842  0.0798256  -0.11754396]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 681\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03066761  0.41301128  0.07747471 -0.3840133 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 682\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03892783  0.21687976  0.06979445 -0.06794374]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 683\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04326543  0.41093525  0.06843557 -0.3378154 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 684\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05148413  0.2149096   0.06167926 -0.02436082]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 685\n",
            "Action Taken : 0\n",
            "Next State: [0.05578233 0.01895982 0.06119205 0.28712714]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 686\n",
            "Action Taken : 1\n",
            "Next State: [0.05616152 0.21315816 0.06693459 0.01435407]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 687\n",
            "Action Taken : 0\n",
            "Next State: [0.06042469 0.01714331 0.06722167 0.32738203]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 688\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06076755 -0.17886807  0.07376931  0.6404833 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 689\n",
            "Action Taken : 1\n",
            "Next State: [0.05719019 0.01515214 0.08657898 0.37191305]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 690\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05749323 -0.18108624  0.09401724  0.6905905 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 691\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0538715  -0.37737834  0.10782905  1.0113287 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 692\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04632394 -0.18384749  0.12805562  0.75435877]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 693\n",
            "Action Taken : 1\n",
            "Next State: [0.04264699 0.00929838 0.1431428  0.50455767]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 694\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04283296 -0.18752001  0.15323395  0.83870625]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 695\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03908256 -0.38436502  0.17000808  1.1753887 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 696\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03139526 -0.1918098   0.19351585  0.9404618 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 697\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02755906 -0.38893887  0.21232508  1.2871753 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.03927939 -0.0043671   0.02184086 -0.03103846]\n",
            "\n",
            "Step 698\n",
            "Action Taken : 0\n",
            "Next State: [-0.03936673 -0.19979535  0.02122009  0.2684547 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 699\n",
            "Action Taken : 1\n",
            "Next State: [-0.04336264 -0.00498256  0.02658918 -0.01746047]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 700\n",
            "Action Taken : 1\n",
            "Next State: [-0.04346229  0.18974818  0.02623997 -0.30163702]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 701\n",
            "Action Taken : 1\n",
            "Next State: [-0.03966733  0.3844865   0.02020723 -0.5859303 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 702\n",
            "Action Taken : 0\n",
            "Next State: [-0.0319776   0.18908744  0.00848863 -0.286951  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 703\n",
            "Action Taken : 0\n",
            "Next State: [-0.02819585 -0.00615453  0.00274961  0.00839703]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 704\n",
            "Action Taken : 0\n",
            "Next State: [-0.02831894 -0.2013158   0.00291755  0.30194622]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 705\n",
            "Action Taken : 0\n",
            "Next State: [-0.03234526 -0.39647922  0.00895647  0.59554785]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 706\n",
            "Action Taken : 0\n",
            "Next State: [-0.04027484 -0.59172535  0.02086743  0.8910385 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 707\n",
            "Action Taken : 0\n",
            "Next State: [-0.05210935 -0.7871241   0.0386882   1.1902074 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 708\n",
            "Action Taken : 0\n",
            "Next State: [-0.06785183 -0.9827255   0.06249235  1.4947613 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 709\n",
            "Action Taken : 1\n",
            "Next State: [-0.08750634 -0.78841674  0.09238757  1.222228  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 710\n",
            "Action Taken : 1\n",
            "Next State: [-0.10327467 -0.59459853  0.11683214  0.95986384]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 711\n",
            "Action Taken : 0\n",
            "Next State: [-0.11516665 -0.79108065  0.1360294   1.2868484 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 712\n",
            "Action Taken : 1\n",
            "Next State: [-0.13098826 -0.59792644  0.16176638  1.039663  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 713\n",
            "Action Taken : 0\n",
            "Next State: [-0.1429468  -0.79478455  0.18255964  1.3784472 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 714\n",
            "Action Taken : 0\n",
            "Next State: [-0.15884247 -0.9916556   0.21012859  1.7222213 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.02919293 -0.01323088  0.00129514  0.03323252]\n",
            "\n",
            "Step 715\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02892831 -0.20837137  0.00195979  0.3263238 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 716\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02476088 -0.01327738  0.00848627  0.03425954]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 717\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02449534 -0.20852     0.00917146  0.32960784]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 718\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02032494 -0.0135298   0.01576361  0.03983122]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 719\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02005434 -0.20887421  0.01656024  0.33744574]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 720\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01587686 -0.01399178  0.02330915  0.05003067]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 721\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01559702 -0.20944007  0.02430977  0.34997573]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 722\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01140822 -0.01467214  0.03130928  0.06505653]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 723\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01111478  0.17998725  0.03261041 -0.21758604]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 724\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01471452 -0.01558533  0.02825869  0.08520263]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 725\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01440281 -0.21110071  0.02996274  0.38666555]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 726\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0101808  -0.4066349   0.03769606  0.6886429 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 727\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0020481  -0.21205585  0.05146891  0.4080617 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 728\n",
            "Action Taken : 1\n",
            "Next State: [-0.00219302 -0.01770001  0.05963015  0.13203919]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 729\n",
            "Action Taken : 1\n",
            "Next State: [-0.00254702  0.17651932  0.06227093 -0.14125113]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 730\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00098337  0.37069672  0.05944591 -0.41365686]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 731\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00839731  0.17478475  0.05117277 -0.10284119]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 732\n",
            "Action Taken : 1\n",
            "Next State: [ 0.011893    0.3691374   0.04911595 -0.37895033]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 733\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01927575  0.1733536   0.04153694 -0.07119449]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 734\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02274282  0.3678562   0.04011305 -0.3504884 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 735\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03009995  0.5623854   0.03310328 -0.6302574 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 736\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04134765  0.36681756  0.02049814 -0.32733575]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 737\n",
            "Action Taken : 0\n",
            "Next State: [ 0.048684    0.17140986  0.01395142 -0.02825976]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 738\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0521122   0.36632898  0.01338623 -0.3165084 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 739\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05943878  0.5612577   0.00705606 -0.6049399 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 740\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07066394  0.7562803  -0.00504274 -0.895392  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 741\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08578954  0.95147026 -0.02295058 -1.1896558 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 742\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10481895  1.146882   -0.0467437  -1.4894432 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 743\n",
            "Action Taken : 0\n",
            "Next State: [ 0.12775658  0.95235944 -0.07653256 -1.2117156 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 744\n",
            "Action Taken : 1\n",
            "Next State: [ 0.14680378  1.1483814  -0.10076687 -1.5273663 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 745\n",
            "Action Taken : 1\n",
            "Next State: [ 0.1697714  1.3445644 -0.1313142 -1.8497236]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 746\n",
            "Action Taken : 1\n",
            "Next State: [ 0.1966627   1.5408647  -0.16830868 -2.1801345 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 747\n",
            "Action Taken : 0\n",
            "Next State: [ 0.22747998  1.347731   -0.21191137 -1.9437771 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.02928741 -0.02744064  0.0290932  -0.02181981]\n",
            "\n",
            "Step 748\n",
            "Action Taken : 0\n",
            "Next State: [-0.02983622 -0.22296748  0.0286568   0.27989852]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 749\n",
            "Action Taken : 0\n",
            "Next State: [-0.03429557 -0.41848624  0.03425477  0.5814802 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 750\n",
            "Action Taken : 0\n",
            "Next State: [-0.0426653  -0.614071    0.04588437  0.8847542 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 751\n",
            "Action Taken : 0\n",
            "Next State: [-0.05494671 -0.8097849   0.06357946  1.1915014 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 752\n",
            "Action Taken : 1\n",
            "Next State: [-0.07114241 -0.6155417   0.08740948  0.919405  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 753\n",
            "Action Taken : 0\n",
            "Next State: [-0.08345325 -0.8117295   0.10579758  1.2382288 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 754\n",
            "Action Taken : 1\n",
            "Next State: [-0.09968784 -0.6181135   0.13056216  0.98047525]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 755\n",
            "Action Taken : 1\n",
            "Next State: [-0.11205011 -0.4249603   0.15017167  0.73148763]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 756\n",
            "Action Taken : 0\n",
            "Next State: [-0.12054931 -0.6218032   0.16480142  1.0674137 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 757\n",
            "Action Taken : 0\n",
            "Next State: [-0.13298537 -0.818676    0.18614969  1.4069543 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 758\n",
            "Action Taken : 0\n",
            "Next State: [-0.1493589  -1.0155555   0.21428877  1.751584  ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.03017805 -0.02977494  0.04449499 -0.01002129]\n",
            "\n",
            "Step 759\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02958255  0.16468158  0.04429456 -0.28834018]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 760\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03287618 -0.03104311  0.03852776  0.01797725]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 761\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03225532  0.16350573  0.0388873  -0.26230508]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 762\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03552543  0.35805163  0.0336412  -0.5424734 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 763\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04268647  0.552685    0.02279174 -0.8243696 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 764\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05374017  0.35725886  0.00630434 -0.5246063 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 765\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06088534  0.5522915  -0.00418778 -0.815296  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 766\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07193118  0.35722718 -0.0204937  -0.52393323]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 767\n",
            "Action Taken : 1\n",
            "Next State: [ 0.07907572  0.55263144 -0.03097237 -0.8230029 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 768\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09012835  0.74816316 -0.04743243 -1.1252642 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 769\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10509161  0.9438735  -0.06993771 -1.4324394 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 770\n",
            "Action Taken : 1\n",
            "Next State: [ 0.12396908  1.1397853  -0.0985865  -1.7461337 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 771\n",
            "Action Taken : 1\n",
            "Next State: [ 0.14676479  1.3358803  -0.13350917 -2.0677853 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 772\n",
            "Action Taken : 1\n",
            "Next State: [ 0.17348239  1.5320846  -0.17486487 -2.398608  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 773\n",
            "Action Taken : 0\n",
            "Next State: [ 0.20412408  1.338871   -0.22283703 -2.164356  ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.01203736 -0.00199045 -0.01755971  0.02404925]\n",
            "\n",
            "Step 774\n",
            "Action Taken : 1\n",
            "Next State: [-0.01207717  0.19337887 -0.01707872 -0.27412182]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 775\n",
            "Action Taken : 1\n",
            "Next State: [-0.00820959  0.38874027 -0.02256116 -0.5721421 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 776\n",
            "Action Taken : 0\n",
            "Next State: [-0.00043478  0.19394183 -0.034004   -0.28665122]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 777\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00344405  0.3895318  -0.03973703 -0.5898618 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 778\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01123469  0.58518696 -0.05153426 -0.8947925 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 779\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02293843  0.7809684  -0.06943011 -1.2032192 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 780\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0385578  0.5868094 -0.0934945 -0.9330784]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 781\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05029399  0.78305995 -0.11215606 -1.2536159 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 782\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06595518  0.5895388  -0.13722838 -0.9980628 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 783\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07774596  0.39649168 -0.15718964 -0.751433  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 784\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08567579  0.2038463  -0.1722183  -0.5120513 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 785\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08975272  0.40092227 -0.18245932 -0.8536745 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 786\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09777116  0.20869343 -0.1995328  -0.6234635 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 787\n",
            "Action Taken : 0\n",
            "Next State: [ 0.10194504  0.01683409 -0.21200208 -0.3996586 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.04759899  0.0027411  -0.03603698  0.00455644]\n",
            "\n",
            "Step 788\n",
            "Action Taken : 1\n",
            "Next State: [-0.04754417  0.19836085 -0.03594585 -0.29927525]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 789\n",
            "Action Taken : 0\n",
            "Next State: [-0.04357695  0.00376922 -0.04193135 -0.01814216]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 790\n",
            "Action Taken : 1\n",
            "Next State: [-0.04350157  0.19946665 -0.0422942  -0.3237545 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 791\n",
            "Action Taken : 0\n",
            "Next State: [-0.03951223  0.00497165 -0.04876929 -0.04470367]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 792\n",
            "Action Taken : 0\n",
            "Next State: [-0.0394128  -0.18941829 -0.04966336  0.23220204]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 793\n",
            "Action Taken : 1\n",
            "Next State: [-0.04320117  0.00637683 -0.04501932 -0.07572354]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 794\n",
            "Action Taken : 0\n",
            "Next State: [-0.04307363 -0.18807179 -0.04653379  0.20242266]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 795\n",
            "Action Taken : 0\n",
            "Next State: [-0.04683506 -0.3824984  -0.04248534  0.4800709 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 796\n",
            "Action Taken : 0\n",
            "Next State: [-0.05448503 -0.5769957  -0.03288392  0.7590666 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 797\n",
            "Action Taken : 0\n",
            "Next State: [-0.06602494 -0.7716494  -0.01770259  1.0412233 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 798\n",
            "Action Taken : 1\n",
            "Next State: [-0.08145794 -0.57629687  0.00312188  0.74303603]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 799\n",
            "Action Taken : 0\n",
            "Next State: [-0.09298387 -0.7714618   0.0179826   1.0366998 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 800\n",
            "Action Taken : 1\n",
            "Next State: [-0.10841311 -0.57658345  0.0387166   0.7497161 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 801\n",
            "Action Taken : 1\n",
            "Next State: [-0.11994477 -0.38201627  0.05371092  0.46946394]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 802\n",
            "Action Taken : 0\n",
            "Next State: [-0.1275851 -0.5778542  0.0631002  0.7785806]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 803\n",
            "Action Taken : 1\n",
            "Next State: [-0.13914219 -0.383654    0.07867181  0.5063992 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 804\n",
            "Action Taken : 0\n",
            "Next State: [-0.14681527 -0.57979125  0.08879979  0.8228008 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 805\n",
            "Action Taken : 1\n",
            "Next State: [-0.15841109 -0.3859891   0.10525581  0.5593158 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 806\n",
            "Action Taken : 1\n",
            "Next State: [-0.16613087 -0.19248976  0.11644212  0.3015612 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 807\n",
            "Action Taken : 0\n",
            "Next State: [-0.16998067 -0.38906217  0.12247335  0.6285798 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 808\n",
            "Action Taken : 1\n",
            "Next State: [-0.17776191 -0.19584306  0.13504495  0.3768393 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 809\n",
            "Action Taken : 0\n",
            "Next State: [-0.18167877 -0.3925987   0.14258173  0.70886827]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 810\n",
            "Action Taken : 0\n",
            "Next State: [-0.18953075 -0.58937746  0.1567591   1.0428183 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 811\n",
            "Action Taken : 0\n",
            "Next State: [-0.2013183  -0.78619426  0.17761546  1.3803223 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 812\n",
            "Action Taken : 1\n",
            "Next State: [-0.21704218 -0.5936777   0.2052219   1.1480353 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 813\n",
            "Action Taken : 1\n",
            "Next State: [-0.22891574 -0.40173817  0.22818261  0.92608017]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.02974834  0.02090183  0.0325212  -0.02095569]\n",
            "\n",
            "Step 814\n",
            "Action Taken : 0\n",
            "Next State: [-0.02933031 -0.17467105  0.03210209  0.28180805]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 815\n",
            "Action Taken : 1\n",
            "Next State: [-0.03282373  0.01997864  0.03773825 -0.00057965]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 816\n",
            "Action Taken : 0\n",
            "Next State: [-0.03242415 -0.17566364  0.03772666  0.30376723]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 817\n",
            "Action Taken : 0\n",
            "Next State: [-0.03593742 -0.37130237  0.043802    0.60810554]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 818\n",
            "Action Taken : 1\n",
            "Next State: [-0.04336347 -0.1768193   0.05596411  0.32953438]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 819\n",
            "Action Taken : 1\n",
            "Next State: [-0.04689986  0.01746318  0.0625548   0.0550118 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 820\n",
            "Action Taken : 1\n",
            "Next State: [-0.04655059  0.211635    0.06365504 -0.21729714]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 821\n",
            "Action Taken : 0\n",
            "Next State: [-0.0423179   0.01566356  0.0593091   0.09476661]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 822\n",
            "Action Taken : 0\n",
            "Next State: [-0.04200462 -0.18025608  0.06120443  0.40555602]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 823\n",
            "Action Taken : 1\n",
            "Next State: [-0.04560975  0.01394694  0.06931555  0.13277978]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 824\n",
            "Action Taken : 1\n",
            "Next State: [-0.04533081  0.20801106  0.07197114 -0.13725491]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 825\n",
            "Action Taken : 1\n",
            "Next State: [-0.04117059  0.40203232  0.06922604 -0.40639213]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 826\n",
            "Action Taken : 0\n",
            "Next State: [-0.03312994  0.2060005   0.0610982  -0.09271248]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 827\n",
            "Action Taken : 0\n",
            "Next State: [-0.02900993  0.01005846  0.05924395  0.21860386]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 828\n",
            "Action Taken : 0\n",
            "Next State: [-0.02880876 -0.18585813  0.06361603  0.5293707 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 829\n",
            "Action Taken : 1\n",
            "Next State: [-0.03252592  0.00831389  0.07420345  0.25739235]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 830\n",
            "Action Taken : 1\n",
            "Next State: [-0.03235964  0.20230234  0.07935129 -0.01099381]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 831\n",
            "Action Taken : 0\n",
            "Next State: [-0.0283136   0.00613734  0.07913142  0.3056326 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 832\n",
            "Action Taken : 0\n",
            "Next State: [-0.02819085 -0.19001785  0.08524407  0.622185  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 833\n",
            "Action Taken : 0\n",
            "Next State: [-0.03199121 -0.3862202   0.09768777  0.94045126]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 834\n",
            "Action Taken : 0\n",
            "Next State: [-0.03971561 -0.58251345  0.11649679  1.262162  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 835\n",
            "Action Taken : 0\n",
            "Next State: [-0.05136588 -0.77891624  0.14174002  1.5889419 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 836\n",
            "Action Taken : 0\n",
            "Next State: [-0.0669442  -0.975409    0.17351887  1.9222574 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 837\n",
            "Action Taken : 1\n",
            "Next State: [-0.08645239 -0.78252417  0.21196403  1.6880339 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00973414 -0.03668719 -0.01175378 -0.04193756]\n",
            "\n",
            "Step 838\n",
            "Action Taken : 0\n",
            "Next State: [ 0.0090004  -0.23163864 -0.01259253  0.2470139 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 839\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00436763 -0.03633913 -0.00765225 -0.04961425]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 840\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00364085 -0.23135053 -0.00864453  0.24064453]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 841\n",
            "Action Taken : 0\n",
            "Next State: [-0.00098617 -0.42634794 -0.00383164  0.53058827]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 842\n",
            "Action Taken : 1\n",
            "Next State: [-0.00951312 -0.2311723   0.00678012  0.23670043]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 843\n",
            "Action Taken : 1\n",
            "Next State: [-0.01413657 -0.03614786  0.01151413 -0.05383615]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 844\n",
            "Action Taken : 0\n",
            "Next State: [-0.01485953 -0.23143299  0.01043741  0.24245723]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 845\n",
            "Action Taken : 0\n",
            "Next State: [-0.01948819 -0.42670247  0.01528655  0.538414  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 846\n",
            "Action Taken : 0\n",
            "Next State: [-0.02802224 -0.622036    0.02605483  0.8358741 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 847\n",
            "Action Taken : 0\n",
            "Next State: [-0.04046296 -0.8175039   0.04277232  1.1366358 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 848\n",
            "Action Taken : 1\n",
            "Next State: [-0.05681304 -0.62296677  0.06550503  0.85766816]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 849\n",
            "Action Taken : 0\n",
            "Next State: [-0.06927237 -0.8189171   0.0826584   1.170208  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 850\n",
            "Action Taken : 1\n",
            "Next State: [-0.08565071 -0.6249616   0.10606255  0.90454197]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 851\n",
            "Action Taken : 0\n",
            "Next State: [-0.09814994 -0.8213477   0.12415339  1.2285898 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 852\n",
            "Action Taken : 0\n",
            "Next State: [-0.1145769  -1.0178292   0.14872518  1.5574508 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 853\n",
            "Action Taken : 1\n",
            "Next State: [-0.13493349 -0.8247683   0.1798742   1.3146206 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 854\n",
            "Action Taken : 1\n",
            "Next State: [-0.15142885 -0.6323198   0.20616661  1.0832036 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 855\n",
            "Action Taken : 1\n",
            "Next State: [-0.16407524 -0.4404256   0.2278307   0.86164236]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.03776946  0.04835276  0.02941081  0.02049546]\n",
            "\n",
            "Step 856\n",
            "Action Taken : 0\n",
            "Next State: [-0.0368024  -0.14717835  0.02982072  0.32231084]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 857\n",
            "Action Taken : 1\n",
            "Next State: [-0.03974597  0.04750653  0.03626693  0.03917932]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 858\n",
            "Action Taken : 1\n",
            "Next State: [-0.03879584  0.24209015  0.03705052 -0.24184404]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 859\n",
            "Action Taken : 0\n",
            "Next State: [-0.03395403  0.04645909  0.03221364  0.06229153]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 860\n",
            "Action Taken : 1\n",
            "Next State: [-0.03302485  0.2411047   0.03345947 -0.22005627]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 861\n",
            "Action Taken : 0\n",
            "Next State: [-0.02820276  0.04552086  0.02905834  0.08299056]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 862\n",
            "Action Taken : 1\n",
            "Next State: [-0.02729234  0.24021447  0.03071816 -0.20038462]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 863\n",
            "Action Taken : 0\n",
            "Next State: [-0.02248805  0.04466696  0.02671046  0.10182799]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 864\n",
            "Action Taken : 0\n",
            "Next State: [-0.02159471 -0.15082741  0.02874702  0.4028169 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 865\n",
            "Action Taken : 1\n",
            "Next State: [-0.02461126  0.04387528  0.03680336  0.11933398]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 866\n",
            "Action Taken : 1\n",
            "Next State: [-0.02373375  0.23845112  0.03919004 -0.1615144 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 867\n",
            "Action Taken : 1\n",
            "Next State: [-0.01896473  0.43299076  0.03595975 -0.44158086]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 868\n",
            "Action Taken : 1\n",
            "Next State: [-0.01030492  0.6275859   0.02712813 -0.72271496]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 869\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0022468   0.8223223   0.01267384 -1.0067375 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 870\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01869325  1.0172727  -0.00746091 -1.2954136 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 871\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0390387   1.2124887  -0.03336918 -1.5904229 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 872\n",
            "Action Taken : 0\n",
            "Next State: [ 0.06328847  1.0177785  -0.06517764 -1.3083289 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 873\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08364405  0.82354015 -0.09134421 -1.0367386 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 874\n",
            "Action Taken : 1\n",
            "Next State: [ 0.10011484  1.0197498  -0.11207899 -1.3566439 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 875\n",
            "Action Taken : 0\n",
            "Next State: [ 0.12050984  0.82619804 -0.13921186 -1.1010202 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 876\n",
            "Action Taken : 1\n",
            "Next State: [ 0.1370338   1.0228497  -0.16123228 -1.4339402 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 877\n",
            "Action Taken : 0\n",
            "Next State: [ 0.15749079  0.83004165 -0.18991108 -1.1956763 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 878\n",
            "Action Taken : 1\n",
            "Next State: [ 0.17409162  1.0270452  -0.2138246  -1.5413677 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.02360608  0.02137723  0.01898367 -0.0391838 ]\n",
            "\n",
            "Step 879\n",
            "Action Taken : 1\n",
            "Next State: [-0.02317853  0.21622188  0.0182     -0.32581726]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 880\n",
            "Action Taken : 1\n",
            "Next State: [-0.0188541   0.41108003  0.01168365 -0.6127056 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 881\n",
            "Action Taken : 0\n",
            "Next State: [-0.0106325   0.21579677 -0.00057046 -0.31636575]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 882\n",
            "Action Taken : 1\n",
            "Next State: [-0.00631656  0.41092685 -0.00689777 -0.60922855]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 883\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00190198  0.60614455 -0.01908235 -0.90407604]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 884\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01402487  0.8015197  -0.03716386 -1.2026953 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 885\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03005526  0.9971019  -0.06121777 -1.5067897 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 886\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0499973   1.1929106  -0.09135357 -1.8179392 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 887\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07385551  0.9989154  -0.12771235 -1.5549805 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 888\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09383382  1.1953155  -0.15881196 -1.8846269 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 889\n",
            "Action Taken : 0\n",
            "Next State: [ 0.11774012  1.0022384  -0.1965045  -1.6451504 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 890\n",
            "Action Taken : 0\n",
            "Next State: [ 0.1377849  0.8098836 -0.2294075 -1.4195722]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.013976   -0.00328556  0.01926223  0.03269176]\n",
            "\n",
            "Step 891\n",
            "Action Taken : 0\n",
            "Next State: [-0.01404171 -0.19867837  0.01991607  0.33138934]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 892\n",
            "Action Taken : 1\n",
            "Next State: [-0.01801528 -0.00384549  0.02654385  0.04505292]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 893\n",
            "Action Taken : 1\n",
            "Next State: [-0.01809219  0.19088598  0.02744491 -0.23913841]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 894\n",
            "Action Taken : 0\n",
            "Next State: [-0.01427447 -0.00461707  0.02266214  0.06207352]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 895\n",
            "Action Taken : 0\n",
            "Next State: [-0.01436681 -0.2000565   0.02390361  0.36181948]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 896\n",
            "Action Taken : 1\n",
            "Next State: [-0.01836794 -0.00528231  0.03114     0.07676866]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 897\n",
            "Action Taken : 1\n",
            "Next State: [-0.01847359  0.1893797   0.03267537 -0.20592913]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 898\n",
            "Action Taken : 0\n",
            "Next State: [-0.01468599 -0.0061939   0.02855679  0.09687954]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 899\n",
            "Action Taken : 1\n",
            "Next State: [-0.01480987  0.18850738  0.03049438 -0.18665874]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 900\n",
            "Action Taken : 0\n",
            "Next State: [-0.01103973 -0.0070373   0.02676121  0.11548586]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 901\n",
            "Action Taken : 1\n",
            "Next State: [-0.01118047  0.18769118  0.02907093 -0.16863541]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 902\n",
            "Action Taken : 0\n",
            "Next State: [-0.00742665 -0.00783456  0.02569822  0.13307492]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 903\n",
            "Action Taken : 0\n",
            "Next State: [-0.00758334 -0.203315    0.02835972  0.43375322]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 904\n",
            "Action Taken : 1\n",
            "Next State: [-0.01164964 -0.00860581  0.03703478  0.1501435 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 905\n",
            "Action Taken : 1\n",
            "Next State: [-0.01182176  0.18596679  0.04003765 -0.13062952]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 906\n",
            "Action Taken : 0\n",
            "Next State: [-0.00810242 -0.00970514  0.03742506  0.17441107]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 907\n",
            "Action Taken : 0\n",
            "Next State: [-0.00829652 -0.20534217  0.04091328  0.47866154]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 908\n",
            "Action Taken : 1\n",
            "Next State: [-0.01240337 -0.01082099  0.05048651  0.19914909]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 909\n",
            "Action Taken : 1\n",
            "Next State: [-0.01261979  0.18354388  0.0544695  -0.07719   ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 910\n",
            "Action Taken : 0\n",
            "Next State: [-0.00894891 -0.01231491  0.05292569  0.23216857]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 911\n",
            "Action Taken : 0\n",
            "Next State: [-0.00919521 -0.2081516   0.05756906  0.54106516]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 912\n",
            "Action Taken : 0\n",
            "Next State: [-0.01335824 -0.40403345  0.06839037  0.85131717]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 913\n",
            "Action Taken : 0\n",
            "Next State: [-0.02143891 -0.6000179   0.08541671  1.1646976 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 914\n",
            "Action Taken : 0\n",
            "Next State: [-0.03343926 -0.79614156  0.10871067  1.4828926 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 915\n",
            "Action Taken : 1\n",
            "Next State: [-0.0493621  -0.60250044  0.13836852  1.2260436 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 916\n",
            "Action Taken : 1\n",
            "Next State: [-0.0614121  -0.40940416  0.16288939  0.9797182 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 917\n",
            "Action Taken : 1\n",
            "Next State: [-0.06960019 -0.21679597  0.18248375  0.74230826]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 918\n",
            "Action Taken : 1\n",
            "Next State: [-0.0739361  -0.02459896  0.19732992  0.51215255]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 919\n",
            "Action Taken : 1\n",
            "Next State: [-0.07442809  0.16727643  0.20757297  0.28756416]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 920\n",
            "Action Taken : 1\n",
            "Next State: [-0.07108255  0.35892728  0.21332425  0.06684798]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.04739705 -0.03823659  0.04555941  0.03152563]\n",
            "\n",
            "Step 921\n",
            "Action Taken : 1\n",
            "Next State: [-0.04816178  0.1562034   0.04618993 -0.24644189]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 922\n",
            "Action Taken : 1\n",
            "Next State: [-0.04503771  0.35063627  0.04126109 -0.52420515]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 923\n",
            "Action Taken : 1\n",
            "Next State: [-0.03802499  0.545154    0.03077699 -0.80360603]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 924\n",
            "Action Taken : 0\n",
            "Next State: [-0.02712191  0.34962386  0.01470487 -0.50140274]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 925\n",
            "Action Taken : 1\n",
            "Next State: [-0.02012943  0.54453546  0.00467681 -0.7894155 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 926\n",
            "Action Taken : 0\n",
            "Next State: [-0.00923872  0.34934962 -0.0111115  -0.49526492]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 927\n",
            "Action Taken : 1\n",
            "Next State: [-0.00225173  0.5446265  -0.0210168  -0.79142886]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 928\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0086408   0.7400306  -0.03684537 -1.0906488 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 929\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02344141  0.93561834 -0.05865835 -1.3946613 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 930\n",
            "Action Taken : 0\n",
            "Next State: [ 0.04215378  0.74127334 -0.08655158 -1.1208808 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 931\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05697925  0.5473865  -0.10896919 -0.8565537 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 932\n",
            "Action Taken : 1\n",
            "Next State: [ 0.06792698  0.7438111  -0.12610027 -1.1814165 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 933\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0828032   0.94032365 -0.1497286  -1.5108203 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 934\n",
            "Action Taken : 0\n",
            "Next State: [ 0.10160967  0.7472992  -0.179945   -1.2683789 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 935\n",
            "Action Taken : 0\n",
            "Next State: [ 0.11655565  0.5548727  -0.20531258 -1.0370184 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 936\n",
            "Action Taken : 0\n",
            "Next State: [ 0.1276531   0.36298284 -0.22605295 -0.81516767]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.00063594 -0.00940021  0.01503722  0.0294969 ]\n",
            "\n",
            "Step 937\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00044794  0.1855029   0.01562716 -0.25840396]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 938\n",
            "Action Taken : 0\n",
            "Next State: [ 0.004158   -0.00983862  0.01045908  0.03916676]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 939\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00396122 -0.20510897  0.01124242  0.3351312 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 940\n",
            "Action Taken : 1\n",
            "Next State: [-0.00014096 -0.01014881  0.01794504  0.04601463]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 941\n",
            "Action Taken : 1\n",
            "Next State: [-0.00034393  0.18471128  0.01886534 -0.2409529 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 942\n",
            "Action Taken : 1\n",
            "Next State: [ 0.00335029  0.37955874  0.01404628 -0.527626  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 943\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01094147  0.184242    0.00349376 -0.23055032]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 944\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01462631 -0.0109297  -0.00111725  0.06323262]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 945\n",
            "Action Taken : 1\n",
            "Next State: [ 1.4407714e-02  1.8420824e-01  1.4740365e-04 -2.2980261e-01]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 946\n",
            "Action Taken : 1\n",
            "Next State: [ 0.01809188  0.3793281  -0.00444865 -0.522439  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 947\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02567844  0.18426904 -0.01489743 -0.23116124]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 948\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02936382 -0.0106369  -0.01952065  0.05678555]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 949\n",
            "Action Taken : 1\n",
            "Next State: [ 0.02915108  0.18475942 -0.01838494 -0.2419918 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 950\n",
            "Action Taken : 1\n",
            "Next State: [ 0.03284627  0.3801391  -0.02322478 -0.54041666]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 951\n",
            "Action Taken : 1\n",
            "Next State: [ 0.04044905  0.5755797  -0.03403311 -0.84032595]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 952\n",
            "Action Taken : 0\n",
            "Next State: [ 0.05196065  0.38093847 -0.05083963 -0.558537  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 953\n",
            "Action Taken : 1\n",
            "Next State: [ 0.05957942  0.57673585 -0.06201037 -0.866794  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 954\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07111414  0.38251007 -0.07934625 -0.59423465]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 955\n",
            "Action Taken : 0\n",
            "Next State: [ 0.07876433  0.18858314 -0.09123094 -0.3275628 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 956\n",
            "Action Taken : 0\n",
            "Next State: [ 0.082536   -0.00512957 -0.0977822  -0.06498683]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 957\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08243341  0.19124845 -0.09908193 -0.38684893]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 958\n",
            "Action Taken : 0\n",
            "Next State: [ 0.08625837 -0.00233764 -0.10681891 -0.12697643]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 959\n",
            "Action Taken : 1\n",
            "Next State: [ 0.08621162  0.19413953 -0.10935844 -0.45135745]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 960\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09009442  0.00072035 -0.11838559 -0.19504918]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 961\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09010882  0.19731925 -0.12228658 -0.52260756]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 962\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09405521  0.00411149 -0.13273872 -0.27082282]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 963\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09413744 -0.18889117 -0.13815518 -0.02277624]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 964\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09035961  0.00791382 -0.1386107  -0.35565946]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 965\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09051789  0.20470634 -0.1457239  -0.68863827]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 966\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09461202  0.01187516 -0.15949667 -0.44514853]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 967\n",
            "Action Taken : 0\n",
            "Next State: [ 0.09484952 -0.1806733  -0.16839963 -0.20668523]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 968\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09123605  0.01640627 -0.17253333 -0.5473987 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 969\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09156418  0.2134786  -0.1834813  -0.8890918 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 970\n",
            "Action Taken : 1\n",
            "Next State: [ 0.09583375  0.41055253 -0.20126314 -1.233382  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 971\n",
            "Action Taken : 1\n",
            "Next State: [ 0.1040448   0.60761064 -0.2259308  -1.5817754 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.02943441  0.03187135  0.04804317 -0.00784371]\n",
            "\n",
            "Step 972\n",
            "Action Taken : 0\n",
            "Next State: [ 0.03007184 -0.1639055   0.04788629  0.299602  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 973\n",
            "Action Taken : 0\n",
            "Next State: [ 0.02679373 -0.35967615  0.05387833  0.6069945 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 974\n",
            "Action Taken : 1\n",
            "Next State: [ 0.0196002  -0.16534728  0.06601822  0.33175677]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 975\n",
            "Action Taken : 0\n",
            "Next State: [ 0.01629326 -0.36134377  0.07265336  0.64450634]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 976\n",
            "Action Taken : 0\n",
            "Next State: [ 0.00906638 -0.55739903  0.08554348  0.95915467]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 977\n",
            "Action Taken : 1\n",
            "Next State: [-0.0020816  -0.3635248   0.10472658  0.69452584]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 978\n",
            "Action Taken : 0\n",
            "Next State: [-0.00935209 -0.55993146  0.1186171   1.0182551 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 979\n",
            "Action Taken : 0\n",
            "Next State: [-0.02055072 -0.75641763  0.13898219  1.345705  ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 980\n",
            "Action Taken : 0\n",
            "Next State: [-0.03567908 -0.9529864   0.1658963   1.6784444 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 981\n",
            "Action Taken : 0\n",
            "Next State: [-0.0547388  -1.149598    0.19946519  2.0178628 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 982\n",
            "Action Taken : 0\n",
            "Next State: [-0.07773076 -1.3461535   0.23982245  2.3651052 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [-0.03413767  0.01430781  0.04099982  0.00364262]\n",
            "\n",
            "Step 983\n",
            "Action Taken : 0\n",
            "Next State: [-0.03385151 -0.18137743  0.04107267  0.30897436]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 984\n",
            "Action Taken : 1\n",
            "Next State: [-0.03747906  0.01313597  0.04725216  0.02952231]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 985\n",
            "Action Taken : 0\n",
            "Next State: [-0.03721634 -0.18263063  0.04784261  0.33673143]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 986\n",
            "Action Taken : 1\n",
            "Next State: [-0.04086896  0.01177899  0.05457724  0.05951103]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 987\n",
            "Action Taken : 0\n",
            "Next State: [-0.04063338 -0.1840813   0.05576745  0.36890176]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 988\n",
            "Action Taken : 0\n",
            "Next State: [-0.044315   -0.37994945  0.06314549  0.67863435]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 989\n",
            "Action Taken : 1\n",
            "Next State: [-0.05191399 -0.18575892  0.07671818  0.40648156]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 990\n",
            "Action Taken : 1\n",
            "Next State: [-0.05562917  0.00819608  0.08484781  0.13893782]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 991\n",
            "Action Taken : 0\n",
            "Next State: [-0.05546525 -0.18803217  0.08762656  0.45713666]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 992\n",
            "Action Taken : 0\n",
            "Next State: [-0.05922589 -0.38427657  0.0967693   0.7761031 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 993\n",
            "Action Taken : 0\n",
            "Next State: [-0.06691142 -0.5805868   0.11229136  1.0975966 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 994\n",
            "Action Taken : 1\n",
            "Next State: [-0.07852316 -0.38710785  0.1342433   0.8421503 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 995\n",
            "Action Taken : 1\n",
            "Next State: [-0.08626532 -0.19404854  0.1510863   0.59451586]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 996\n",
            "Action Taken : 1\n",
            "Next State: [-0.09014629 -0.00132811  0.16297662  0.35297897]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 997\n",
            "Action Taken : 0\n",
            "Next State: [-0.09017285 -0.19834696  0.1700362   0.6922944 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 998\n",
            "Action Taken : 0\n",
            "Next State: [-0.09413978 -0.3953688   0.18388209  1.0333153 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 999\n",
            "Action Taken : 0\n",
            "Next State: [-0.10204716 -0.5923964   0.20454839  1.3776314 ]\n",
            "Reward : 1.0\n",
            "Terminated: False, Truncated : False\n",
            "\n",
            "Step 1000\n",
            "Action Taken : 1\n",
            "Next State: [-0.11389509 -0.40033057  0.23210102  1.1552575 ]\n",
            "Reward : 1.0\n",
            "Terminated: True, Truncated : False\n",
            "\n",
            "Episode finished - resetting environment.\n",
            "Now Initial State : [ 0.02791769 -0.00426567  0.00641137  0.02423312]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"MountainCar-v0\",render_mode=\"human\")\n",
        "state, info = env.reset()\n",
        "print(f\"Initial State: {state}\")\n",
        "for step in range(300):\n",
        "  action = env.action_space.sample()\n",
        "  next_state, reward, terminated, truncated, info = env.step(action)\n",
        "  print(f\"\\nStep : {step+1}\")\n",
        "  print(f\"Action : {action}, Reward : {reward}\")\n",
        "  print(f\"Next State : {next_state}\")\n",
        "  print(f\"Terminated : {terminated}, Truncated : {truncated}\")\n",
        "  if terminated:\n",
        "    print(\"Goal reached! Episode ended naturally.\")\n",
        "    state, info = env.reset()\n",
        "  elif truncated:\n",
        "    print(\"Time limit reached. Episode truncated.\")\n",
        "    state, info = env.reset()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlGegLbN-aKf",
        "outputId": "e05ef7ac-1e61-495e-c21b-1dddc07b3114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State: [-0.43772382  0.        ]\n",
            "\n",
            "Step : 1\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-4.3736076e-01  3.6303856e-04]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 2\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.4366373   0.00072344]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 3\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.4375587  -0.00092139]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 4\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.43911827 -0.00155955]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 5\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.44230467 -0.00318639]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 6\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.44509473 -0.00279007]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 7\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.44846815 -0.00337343]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 8\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4524003  -0.00393215]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 9\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.45786238 -0.00546208]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 10\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.46381432 -0.00595192]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 11\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.4712122 -0.0073979]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 12\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.4780014  -0.00678919]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 13\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.4861315 -0.0081301]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 14\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.49554202 -0.00941051]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 15\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5061627  -0.01062069]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 16\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5179141 -0.0117514]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 17\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.52870816 -0.01079404]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 18\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5394639  -0.01075572]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 19\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5511006  -0.01163677]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 20\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.56153136 -0.01043074]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 21\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5716782  -0.01014686]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 22\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.58246577 -0.01078751]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 23\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.592814  -0.0103483]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 24\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.60264695 -0.00983291]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 25\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.61289257 -0.01024559]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 26\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6224764  -0.00958387]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 27\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.63132954 -0.00885312]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 28\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6393887  -0.00805913]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 29\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.64559674 -0.00620806]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 30\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6519101  -0.00631337]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 31\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6582847  -0.00637462]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 32\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.66367644 -0.00539174]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 33\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.66904825 -0.00537181]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 34\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6733635  -0.00431523]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 35\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.67759293 -0.0042294 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 36\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.680708   -0.00311509]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 37\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6836879  -0.00297992]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 38\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6855128  -0.00182488]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 39\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6871705  -0.00165772]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 40\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6886501  -0.00147957]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 41\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.68994176 -0.00129166]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 42\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.69103694 -0.00109522]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 43\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-6.9092858e-01  1.0841465e-04]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 44\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6886172   0.00231134]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 45\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6861182   0.00249904]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 46\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.681448    0.00467022]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 47\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.67663765  0.00481033]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 48\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6717194   0.00491822]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 49\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6647265   0.00699293]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 50\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.65670645  0.00802005]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 51\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6467144   0.00999203]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 52\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.63481987  0.01189455]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 53\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6211065   0.01371331]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 54\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6066723   0.01443424]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 55\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.59162146  0.01505086]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 56\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5770639   0.01455749]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 57\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5631072   0.01395677]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 58\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5478548   0.01525239]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 59\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.53142065  0.01643415]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 60\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5139278  0.0174928]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 61\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.49550757  0.01842028]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 62\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4772977   0.01820985]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 63\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.460434    0.01686371]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 64\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4440412  0.0163928]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 65\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.42923942  0.01480177]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 66\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.41513595  0.01410349]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 67\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.40083167  0.01430429]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 68\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.38642743  0.01440421]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 69\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.37402323  0.01240422]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 70\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.36170354  0.01231968]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 71\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.35055092  0.01115264]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 72\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3406386  0.0099123]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 73\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.33203065  0.00860796]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 74\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.32378164  0.008249  ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 75\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.31594312  0.00783852]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 76\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.3085632   0.00737991]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 77\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.30368653  0.0048767 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 78\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.3013421   0.00234442]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 79\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-3.0154380e-01 -2.0170324e-04]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 80\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.30329043 -0.00174664]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 81\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3065717  -0.00328126]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 82\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.31136808 -0.00479638]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 83\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3176508  -0.00628272]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 84\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.32538173 -0.00773092]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 85\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3345132  -0.00913147]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 86\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.34598798 -0.01147477]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 87\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.35873267 -0.01274471]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 88\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3726641  -0.01393141]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 89\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.38868922 -0.01602513]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 90\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.40569878 -0.01700955]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 91\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.42357424 -0.01787548]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 92\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4421887  -0.01861446]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 93\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.46240768 -0.02021898]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 94\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.483083   -0.02067534]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 95\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.50506145 -0.02197846]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 96\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5261789  -0.02111742]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 97\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.547277   -0.02109807]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 98\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.56719756 -0.01992063]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 99\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5857922  -0.01859457]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 100\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.60392296 -0.01813082]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 101\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6224572 -0.0185342]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 102\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.64026076 -0.01780359]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 103\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.65620714 -0.01594638]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 104\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.67018497 -0.01397784]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 105\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6820985  -0.01191354]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 106\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6928676  -0.01076908]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 107\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.702421   -0.00955343]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 108\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.7096967  -0.00727567]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 109\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.71664804 -0.00695132]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 110\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.72123104 -0.00458304]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 111\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.7234172  -0.00218613]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 112\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-7.2319287e-01  2.2435229e-04]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 113\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-7.225594e-01  6.334447e-04]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 114\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7205208   0.00203861]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 115\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7170897  0.0034311]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 116\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.71328753  0.00380215]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 117\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7081383   0.00514927]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 118\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7016746   0.00646369]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 119\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.69293797  0.00873663]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 120\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6829852   0.00995274]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 121\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6718821   0.01110311]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 122\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6597032   0.01217891]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 123\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6475316   0.01217157]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 124\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6344518  0.0130798]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 125\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6205559   0.01389595]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 126\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6049429   0.01561292]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 127\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.587726    0.01721697]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 128\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.570031    0.01769496]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 129\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5509889   0.01904208]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 130\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.53274167  0.01824727]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 131\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5134258   0.01931584]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 132\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.49518627  0.01823955]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 133\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.47815958  0.01702671]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 134\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4614726   0.01668697]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 135\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.4462489   0.01522372]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 136\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.43260008  0.01364879]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 137\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.41862535  0.01397474]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 138\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.40542495  0.01320039]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 139\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.39309242  0.01233254]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 140\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.38171387  0.01137856]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 141\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3713676   0.01034626]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 142\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.3611238   0.01024381]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 143\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.35205087  0.00907292]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 144\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.3452085   0.00684236]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 145\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.3406411   0.00456739]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 146\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.33737803  0.00326308]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 147\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.33544007  0.00193795]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 148\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.33483958  0.00060051]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 149\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.33658028 -0.00174072]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 150\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.3386512  -0.00207092]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 151\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.34203917 -0.00338795]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 152\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.34572247 -0.00368331]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 153\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.3496774  -0.00395496]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 154\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.3558784  -0.00620098]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 155\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.36228487 -0.00640648]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 156\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.36885455 -0.00656966]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 157\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.37654355 -0.007689  ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 158\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.3853     -0.00875648]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 159\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.39406425 -0.00876422]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 160\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4037757  -0.00971146]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 161\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.41536662 -0.01159091]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 162\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.42775506 -0.01238846]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 163\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.44185248 -0.01409742]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 164\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.45555687 -0.01370439]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 165\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.47076806 -0.01521117]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 166\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.48637378 -0.01560574]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 167\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5022581  -0.01588435]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 168\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.51830244 -0.0160443 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 169\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.53338647 -0.01508402]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 170\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.54839706 -0.01501062]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 171\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5622219 -0.0138248]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 172\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.57475764 -0.01253578]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 173\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.58691126 -0.01215359]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 174\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.59759283 -0.0106816 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 175\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.60872406 -0.01113119]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 176\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6202237  -0.01149967]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 177\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6320088  -0.01178509]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 178\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.64399505 -0.01198627]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 179\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.65409786 -0.0101028 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 180\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.66424674 -0.01014887]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 181\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6743718  -0.01012503]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 182\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.68340415 -0.0090324 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 183\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6902834  -0.00687925]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 184\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.69696397 -0.00668057]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 185\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.70340216 -0.00643816]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 186\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.70855623 -0.00515408]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 187\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.7133932 -0.004837 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 188\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.7158824  -0.00248922]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 189\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7170082  -0.00112575]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 190\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.7157634   0.00124479]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 191\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.7141559   0.00160751]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 192\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.7101958   0.00396011]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 193\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.7059082   0.00428763]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 194\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6993204   0.00658779]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 195\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.69247484  0.00684551]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 196\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.68341625  0.00905859]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 197\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.67220443  0.01121182]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 198\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.66091466  0.01128981]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 199\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6496239   0.01129079]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 200\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.63641024  0.01321362]\n",
            "Terminated : False, Truncated : True\n",
            "Time limit reached. Episode truncated.\n",
            "\n",
            "Step : 201\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.43477583 -0.00167039]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 202\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.43810454 -0.00332871]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 203\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.44306746 -0.00496291]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 204\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4486285  -0.00556104]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 205\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.45574707 -0.00711858]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 206\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.46237105 -0.00662397]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 207\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.47045165 -0.0080806 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 208\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.47992915 -0.00947751]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 209\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.49073327 -0.0108041 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 210\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.50178343 -0.01105019]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 211\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.51199716 -0.01021369]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 212\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.52329785 -0.01130068]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 213\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.53460073 -0.01130294]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 214\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5458212  -0.01122044]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 215\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5578751 -0.0120539]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 216\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.56967235 -0.01179728]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 217\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.5821252  -0.01245282]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 218\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5941413  -0.01201613]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 219\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6046323 -0.010491 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 220\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.61452156 -0.00988922]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 221\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.62473726 -0.01021572]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 222\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.634206   -0.00946877]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 223\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6438604  -0.00965435]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 224\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6516322  -0.00777183]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 225\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.65746725 -0.00583502]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 226\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.662325   -0.00485778]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 227\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6651721  -0.00284711]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 228\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.66598904 -0.00081694]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 229\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.66477025  0.0012188 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 230\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.66252404  0.00224622]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 231\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.66026574  0.00225826]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 232\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.65801096  0.00225478]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 233\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6557752   0.00223577]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 234\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6525739   0.00320132]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 235\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.6484292   0.00414468]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 236\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.64237005  0.00605918]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 237\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6344388   0.00793123]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 238\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6266915  0.0077473]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 239\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.61718327  0.00950823]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 240\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.60698235  0.01020093]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 241\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.59716254  0.0098198 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 242\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.58579546  0.01136706]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 243\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.57296467  0.01283083]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 244\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.55876493  0.01419973]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 245\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.54330194  0.01546298]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 246\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.52669126  0.01661067]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 247\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5100574   0.01663386]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 248\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.49252507  0.01753233]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 249\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.47422546  0.01829962]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 250\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.4552948   0.01893067]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 251\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.43587282  0.01942197]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 252\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.41610125  0.01977159]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 253\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.39712197  0.01897926]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 254\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.3780687   0.01905328]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 255\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.35907254  0.01899615]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 256\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.34026083  0.0188117 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 257\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.3217559   0.01850495]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 258\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.30367392  0.01808196]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 259\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.2861243   0.01754962]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 260\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.27020887  0.01591544]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 261\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.25701603  0.01319283]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 262\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.24561615  0.01139988]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 263\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.23706774  0.0085484 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 264\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.23041329  0.00665446]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 265\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.22568497  0.00472832]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 266\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.2239052   0.00177977]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 267\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.22508232 -0.00117712]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 268\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.22921082 -0.0041285 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 269\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.2342712  -0.00506038]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 270\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.24123915 -0.00696795]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 271\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.25008047 -0.00884132]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 272\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.2607506  -0.01067013]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 273\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.27419403 -0.01344345]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 274\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.28933832 -0.01514428]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 275\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.30609846 -0.01676013]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 276\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.32337654 -0.01727807]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 277\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.3430676  -0.01969105]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 278\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.36404738 -0.02097981]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 279\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.38617867 -0.02213127]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 280\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.41031164 -0.02413297]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 281\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.434278   -0.02396637]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 282\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.4589063  -0.02462828]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 283\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.48401672 -0.02511044]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 284\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.5084233  -0.02440661]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 285\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5329437  -0.02452038]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 286\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.55639404 -0.0234503 ]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 287\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.5795987  -0.02320474]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 288\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.60338545 -0.02378671]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 289\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6255795  -0.02219401]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 290\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.6480205  -0.02244103]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 291\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.6685499  -0.02052938]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 292\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.68902606 -0.02047619]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 293\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.70931184 -0.02028579]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 294\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.7272757  -0.01796389]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 295\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.74280536 -0.01552961]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 296\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7568069  -0.01400152]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 297\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.7701986 -0.0133917]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 298\n",
            "Action : 2, Reward : -1.0\n",
            "Next State : [-0.78090495 -0.01070635]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 299\n",
            "Action : 0, Reward : -1.0\n",
            "Next State : [-0.7908675  -0.00996257]\n",
            "Terminated : False, Truncated : False\n",
            "\n",
            "Step : 300\n",
            "Action : 1, Reward : -1.0\n",
            "Next State : [-0.7990335  -0.00816604]\n",
            "Terminated : False, Truncated : False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAJmP_ZSH0zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}